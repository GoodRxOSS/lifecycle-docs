{"/docs/features/auto-deployment":{"title":"Auto Deploy & Labels","data":{"auto-deploy-configuration#Auto-Deploy Configuration":"To enable automatic deployment when a PR is created, set the autoDeploy attribute in your repository's lifecycle.yaml file:\nLifecycle will automatically create the environment as soon as a PR is opened.\nA lifecycle-deploy! label will be added to the PR to indicate that the environment has been deployed.","managing-deployments-with-labels#Managing Deployments with Labels":"If auto-deploy is not enabled, you can manually control the environment using PR labels.","deploy-an-environment#Deploy an Environment":"To create an ephemeral environment for a PR, add the lifecycle-deploy! label.","tear-down-an-environment#Tear Down an Environment":"To delete an active environment, use either of these labels:\nRemove lifecycle-deploy!\nAdd lifecycle-disabled!","automatic-cleanup-on-pr-closure#Automatic Cleanup on PR Closure":"When a PR is closed, Lifecycle will:\nTear down the active environment.\nRemove the lifecycle-deploy! label from the PR.\nThis ensures that unused environments do not persist after the PR lifecycle is complete.","summary#Summary":"Feature\tBehavior\tautoDeploy: true in config\tPR environments are automatically deployed.\tlifecycle-deploy!\tManually deploy an environment.\tRemove lifecycle-deploy!\tTear down the environment.\tAdd lifecycle-disabled!\tTear down the environment manually.\tPR closed\tEnvironment is deleted automatically.\t\nUsing these configurations and labels, teams can efficiently manage ephemeral environments in their development workflow."}},"/docs/features/ai-agent":{"title":"AI Agent","data":{"":"The AI Agent is an interactive assistant built into Lifecycle that helps you debug and investigate issues in your ephemeral environments. It inspects Kubernetes resources, reads pod logs, queries the Lifecycle database, browses your GitHub repository, and can apply fixes directly.\nAn administrator must enable the AI Agent before you can use it. If the\nfeature is not enabled, you see an \"AI Agent Not Enabled\" message with a\nprompt to contact your administrator. See AI Agent\nConfiguration for setup details.","accessing-the-agent#Accessing the agent":"Navigate to the AI Agent from any build's detail view:\nThe chat interface is scoped to that build's environment ‚Äî the agent already knows which namespace, services, and resources to inspect.","debugging-a-failing-environment#Debugging a failing environment":"Here's a typical debugging session:\nOpen the AI Agent page for your build.\nClick one of the suggested prompts ‚Äî for example, \"Why are my pods not starting?\"\nThe agent queries Kubernetes for pod statuses, reads recent logs, and checks deployment configurations.\nIt returns a structured response with a summary, per-service findings, and suggested fixes.\nAt this point, you should see a response with service cards and an activity panel (described below). If the agent suggests a fix, you can apply it with one click.\nYou don't need to know which Kubernetes commands to run. Describe your problem\nin plain language and let the agent investigate.\nThe suggested prompts on first load are:\n\"Why is my build failing?\"\n\"What's wrong with deployments?\"\n\"Why are my pods not starting?\"\nYou can also type your own question.","understanding-responses#Understanding responses":"The agent returns two response formats depending on the question:\nPlain text ‚Äî for simple or conversational answers\nStructured investigation ‚Äî for debugging queries, containing the sections below","summary#Summary":"A brief overview at the top describing overall environment health and key findings.","service-cards#Service cards":"Each service gets its own card with:\nStatus chip ‚Äî service health: healthy, degraded, failing, pending, or unknown\nIssue ‚Äî root cause or current problem\nErrors ‚Äî specific error messages from logs or events\nSuggested fixes ‚Äî actionable steps you can follow\nAuto-fix button ‚Äî applies a fix directly (for example, restarting a deployment or committing a config change to your PR branch)\nEvidence ‚Äî links back to the tool calls that support the findings","activity-panel#Activity panel":"Each response includes a collapsible Investigation panel showing every tool call the agent made:\nStatus icon ‚Äî spinner (in progress), green checkmark (success), or red X (failure)\nDescription ‚Äî what the agent did (e.g., \"Fetched pods in namespace env-abc123\")\nDuration chip ‚Äî how long the call took\nA total investigation time appears at the top of the panel. Evidence references in the response link to specific entries in this panel.","selecting-a-model#Selecting a model":"A Model dropdown in the chat header displays available models in the format provider:modelId (for example, \"Claude Sonnet\" or \"GPT-4o\"). Your selection is saved to localStorage and persists across sessions.A Clear button appears when you have messages. It clears the conversation history and starts a new session.","supported-providers#Supported providers":"Each provider requires its own API key configured on the server:\nProvider\tEnvironment Variable\tFallback Variable\tAnthropic\tANTHROPIC_API_KEY\tAI_API_KEY\tOpenAI\tOPENAI_API_KEY\tAI_API_KEY\tGemini\tGEMINI_API_KEY\tAI_API_KEY\t\nSet AI_API_KEY as a universal fallback if all providers share the same key.\nProvider-specific keys take precedence.","agent-capabilities#Agent capabilities":"The agent has access to tools organized by category:\nCategory\tTools\tWhat they do\tKubernetes\tGet Resources, Get Pod Logs, Get Lifecycle Logs\tInspect pods, deployments, services, events, and logs in the build namespace\tKubernetes\tPatch Resource\tRestart deployments, scale replicas, delete stuck resources, apply patches\tKubernetes\tQuery Database\tRun read-only queries against the Lifecycle database for build and deploy metadata\tGitHub\tGet File, List Directory\tBrowse repository files and directories on the PR branch\tGitHub\tUpdate File\tCommit a fix directly to the PR branch\tGitHub\tGet Issue Comment\tRead PR comments for additional context\t\nAdministrators can restrict which tools the agent uses per repository. See AI\nAgent Configuration for\ndetails.\nYou can also extend the agent with external tools via MCP Integration.","summary-1#Summary":"Feature\tDetails\tAccess URL\t/builds/<build-uuid>/ai\tPrerequisite\tMust be enabled by an administrator\tSuggested prompts\t\"Why is my build failing?\", \"What's wrong with deployments?\", \"Why are my pods not starting?\"\tResponse format\tPlain text or structured investigation with service cards\tActivity panel\tCollapsible list of tool calls with status and duration\tModel selection\tDropdown in header, persisted in localStorage\tSupported providers\tAnthropic, OpenAI, Gemini\tAuto-fix\tOne-click fixes via Kubernetes patches or GitHub commits"}},"/docs/features/environment-ttl":{"title":"Environment TTL","data":{"":"Lifecycle automatically cleans up inactive ephemeral environments based on configurable TTL (Time To Live) settings. This helps reduce resource usage while allowing teams to keep environments alive when needed.","configuration#Configuration":"The TTL cleanup configuration is stored in the global_config table with the key ttl_cleanup:","enabled#enabled":"Enables or disables automatic cleanup of inactive environments.Default: false","dryrun#dryRun":"Runs cleanup process without actually tearing down environments. Useful for testing.Default: true","inactivitydays#inactivityDays":"Number of days of inactivity before an environment is eligible for cleanup.Default: 14","checkintervalminutes#checkIntervalMinutes":"How frequently the cleanup job runs to check for stale environments.Default: 240 (4 hours)","commenttemplate#commentTemplate":"Custom message posted to the PR when cleaning up. Supports {inactivityDays} placeholder and dynamic label replacements.Default: \"Tearing down lifecycle env since no activity in the past {inactivityDays} days.\"","excludedrepositories#excludedRepositories":"Repository names (format: owner/repo) to exclude from automatic cleanup.Default: []\nChanges to TTL configuration require refreshing the global config cache or\nwaiting for the automatic refresh cycle.","how-it-works#How It Works":"Lifecycle tracks environment expiration using lfc/ttl-expireAtUnix labels on Kubernetes namespaces. The cleanup service runs periodically to scan for expired environments, validates eligibility (PR is open, not static, not excluded), and tears down environments by removing the deploy label and adding the disabled label.","preventing-cleanup#Preventing Cleanup":"Add the keep label to a PR to prevent automatic cleanup. This disables TTL on the namespace and keeps the environment alive indefinitely. See Configurable Labels to customize the keep label.","summary#Summary":"Property\tType\tPurpose\tenabled\tboolean\tEnable/disable automatic cleanup\tdryRun\tboolean\tTest mode without actual cleanup\tinactivityDays\tnumber\tDays of inactivity before cleanup\tcheckIntervalMinutes\tnumber\tCleanup job frequency in minutes\tcommentTemplate\tstring\tCustom cleanup message for PRs\texcludedRepositories\tstring[]\tRepositories excluded from cleanup"}},"/docs/features/configurable-labels":{"title":"Configurable Labels","data":{"":"Lifecycle allows you to customize the GitHub labels that control environment deployments, TTL behavior, and status comments through the global_config.labels configuration.","configuration#Configuration":"The labels configuration is stored in the global_config table with the key labels:","deploy#deploy":"Labels that trigger environment deployment. When any deploy label is added to a PR, Lifecycle creates an ephemeral environment. The first label in the array is used when adding labels programmatically. Supports multiple labels for different teams or naming conventions.Default: [\"lifecycle-deploy!\"]","disabled#disabled":"Labels that prevent or teardown deployments. When any disabled label is added or deploy label is removed, the active environment will be torn down.Default: [\"lifecycle-disabled!\"]","keep#keep":"Labels that disable TTL cleanup for an environment. When present on a PR, prevents automatic teardown due to inactivity and sets lfc/ttl-enable: false on the Kubernetes namespace.Default: [\"lifecycle-keep!\"]","statuscomments#statusComments":"Labels that control status comment visibility on a per-PR basis. Behavior depends on the defaultStatusComments setting.Default: [\"lifecycle-status-comments!\"]","defaultstatuscomments#defaultStatusComments":"Controls whether status comments are enabled by default across all PRs. When true, status comments show for all PRs. When false, status comments only show when the status comment label is present.Default: true","defaultcontrolcomments#defaultControlComments":"Enables or disables control-related comments system-wide.Default: true\nChanges to label configuration require refreshing the global config cache or\nwaiting for the automatic refresh cycle.","summary#Summary":"Property\tType\tPurpose\tdeploy\tstring[]\tTrigger environment deployment\tdisabled\tstring[]\tPrevent or teardown deployments\tkeep\tstring[]\tDisable TTL cleanup\tstatusComments\tstring[]\tControl status comment visibility\tdefaultStatusComments\tboolean\tEnable status comments by default\tdefaultControlComments\tboolean\tEnable control comments system-wide"}},"/docs/features/mcp-integration":{"title":"MCP Integration","data":{"":"The Model Context Protocol (MCP) is an open standard that lets AI agents discover and call tools from external servers. Lifecycle supports MCP, so you can extend the AI Agent with tools beyond the built-in Kubernetes and GitHub capabilities.When you register an MCP server, Lifecycle connects to it, discovers the available tools, caches their definitions, and makes them available during chat sessions. MCP tools appear alongside built-in tools and work the same way.\nMCP servers must be reachable from the Lifecycle server at the configured URL.\nLifecycle validates connectivity whenever you create or update a server\nconfiguration.","configuring-mcp-servers#Configuring MCP servers":"MCP server configurations are managed through the REST API. Each configuration includes these fields:\nField\tType\tRequired\tDefault\tDescription\tslug\tstring\tYes\t‚Äî\tUnique identifier. Lowercase alphanumeric and hyphens, 1‚Äì100 chars, no leading/trailing hyphens.\tname\tstring\tYes\t‚Äî\tHuman-readable display name.\turl\tstring\tYes\t‚Äî\tFull URL of the MCP server endpoint.\tscope\tstring\tYes\t\"global\"\tEither \"global\" or a repo full name (e.g. \"myorg/myrepo\").\tdescription\tstring\tNo\tnull\tOptional description of the server's purpose.\theaders\tobject\tNo\t{}\tHTTP headers sent with every request (e.g. auth tokens).\tenvVars\tobject\tNo\t{}\tEnvironment variables associated with this server.\tenabled\tboolean\tNo\ttrue\tWhether the server is active. Disabled servers are excluded from tool resolution.\ttimeout\tnumber\tNo\t30000\tTimeout in milliseconds for handshake and tool calls.\tcachedTools\tMcpCachedTool[]\tAuto\t[]\tAuto-populated during connectivity validation with discovered tool definitions.","transport-auto-negotiation#Transport auto-negotiation":"Lifecycle automatically negotiates the transport protocol. It tries StreamableHTTP first, then falls back to SSE (Server-Sent Events). If both fail, the connection is rejected with an error describing what went wrong.","connectivity-validation#Connectivity validation":"Lifecycle checks that the MCP server is reachable and returns at least one tool:\nOn create ‚Äî Validation always runs. If the server is unreachable or returns zero tools, the request fails with 422.\nOn update ‚Äî Validation only runs when url or headers change. If neither changes, the existing cached tools are preserved.\nDuring validation, cachedTools is automatically populated with the tools discovered from the server.","scoping-and-merge-behavior#Scoping and merge behavior":"MCP servers can be scoped at two levels:\nGlobal (scope: \"global\") ‚Äî Available to all repositories.\nPer-repository (scope: \"myorg/myrepo\") ‚Äî Available only to that specific repository.","runtime-merge#Runtime merge":"When the AI Agent starts a session, Lifecycle resolves the effective set of MCP servers by combining:\nAll enabled global servers (minus any in the repository's disabledSlugs list)\nAll enabled per-repository servers for that build's repository\nThe combined list gives the agent the full set of MCP tools.","disabling-global-servers-per-repository#Disabling global servers per-repository":"A repository can opt out of specific global MCP servers by adding their slugs to the disabledSlugs list. This prevents those servers from contributing tools for that repository without affecting others.\nFor per-repository AI Agent settings (like disabledSlugs, excluded tools,\nand custom rules), see AI Agent\nConfiguration.","managing-mcp-servers-via-api#Managing MCP servers via API":"All endpoints are under /api/v2/ai/config/mcp-servers. Request and response bodies use JSON. Header values are redacted to \"******\" in all responses.","list-servers#List servers":"Returns all MCP server configurations for the given scope. Defaults to \"global\" if scope is omitted.","create-a-server#Create a server":"Registers a new MCP server. Lifecycle validates connectivity before saving.\nReturns the created configuration (status 201) with redacted headers and populated cachedTools.\nError\tCause\t400\tMissing required fields (slug, name, url)\t409\tA server with the same slug already exists in the same scope\t422\tSlug validation failed, or server unreachable / returned zero tools","get-a-server-by-slug#Get a server by slug":"Returns a single configuration with redacted headers. Defaults to \"global\" scope. Returns 404 if not found.","update-a-server#Update a server":"Only include the fields you want to change. If url or headers change, connectivity is re-validated.\nReturns the updated configuration. Returns 404 if not found, 422 if connectivity validation fails.","delete-a-server#Delete a server":"Soft-deletes the configuration. Returns 204 on success, 404 if not found.","health-check#Health check":"Checks connectivity for all enabled servers. Connects to each one, lists tools, and reports reachability. If the tool list changed since the last check, cached tools are automatically refreshed.","tool-naming-and-safety#Tool naming and safety":"","naming-convention#Naming convention":"To avoid collisions between servers, MCP tools are exposed with a namespaced name:\nFor example, a tool called lookup_user from a server with slug my-server becomes mcp__my-server__lookup_user. The agent handles this automatically.","safety-mapping#Safety mapping":"MCP tool annotations are mapped to Lifecycle safety levels that influence how the agent uses each tool:\nMCP Annotation\tLifecycle Safety Level\tWhat happens\tdestructiveHint: true\tDANGEROUS\tDescription prefixed with [DANGEROUS]\treadOnlyHint: true\tSAFE\tDescription prefixed with [SAFE]\topenWorldHint: true\tCAUTIOUS\tNo prefix added\tNo annotations\tCAUTIOUS\tDefault safety level\t\nAnnotations are evaluated in priority order: destructiveHint takes precedence over readOnlyHint, which takes precedence over openWorldHint.","error-handling#Error handling":"MCP tool errors fall into three categories:\nError Code\tTrigger\tRecoverable\tMCP_CONNECTION_ERROR\tServer unreachable, timeout, connection refused\tYes\tMCP_TOOL_ERROR\tTool returned an error result\tYes\tMCP_PROTOCOL_ERROR\tJSON-RPC or protocol-level failure\tNo\t\nRecoverable errors let the agent retry or suggest alternatives. Protocol errors indicate a fundamental compatibility issue between Lifecycle and the server.","troubleshooting#Troubleshooting":"","server-fails-connectivity-check#Server fails connectivity check":"Check the URL ‚Äî Confirm the URL is correct and reachable from the Lifecycle server.\nCheck the transport ‚Äî Your server must support StreamableHTTP or SSE. Stdio-only servers do not work with Lifecycle.\nCheck authentication ‚Äî If your server requires auth headers, make sure they are included in the headers field. Header values are redacted in responses, so you cannot read them back.\nCheck the timeout ‚Äî If the server is slow to respond, increase the timeout value.","tools-not-appearing-in-the-agent#Tools not appearing in the agent":"Verify the server returns at least one tool ‚Äî Lifecycle rejects servers that expose zero tools during validation.\nCheck the enabled flag ‚Äî Disabled servers are excluded from tool resolution.\nCheck the scope ‚Äî A server scoped to a specific repo does not appear for builds in other repos.\nCheck disabledSlugs ‚Äî A repo-level configuration may be opting out of your global server.","health-check-shows-unreachable#Health check shows unreachable":"Run the health check endpoint to see the specific error message in the error field. Common causes include the server being down or network policies blocking traffic from Lifecycle pods.\nUse GET /api/v2/ai/config/mcp-servers/health to verify all servers are\nreachable and their tool caches are current."}},"/docs/features/native-helm-deployment":{"title":"Native Helm Deployment","data":{"":"This feature is still in alpha and might change with breaking changes.\nNative Helm is an alternative deployment method that runs Helm deployments directly within Kubernetes jobs, eliminating the need for external CI/CD systems. This provides a more self-contained and portable deployment solution.\nNative Helm deployment is an opt-in feature that can be enabled globally or\nper-service.","overview#Overview":"When enabled, Native Helm:\nCreates Kubernetes jobs to execute Helm deployments\nRuns in ephemeral namespaces with proper RBAC\nProvides real-time deployment logs via WebSocket\nHandles concurrent deployments automatically\nSupports all standard Helm chart types","quickstart#Quickstart":"Want to try native Helm deployment? Here's the fastest way to get started:\nThis configuration:\nEnables native Helm for the my-api service\nUses a local Helm chart from your repository\nApplies values from ./helm/values.yaml\nRuns deployment as a Kubernetes job\nTo enable native Helm for all services at once, see Global\nConfiguration.","configuration#Configuration":"","enabling-native-helm#Enabling Native Helm":"There are two ways to enable native Helm deployment:","per-service-configuration#Per Service Configuration":"Enable native Helm for individual services:","global-configuration#Global Configuration":"Enable native Helm for all services:","configuration-precedence#Configuration Precedence":"Lifecycle uses a hierarchical configuration system with three levels of precedence:\nhelmDefaults - Base defaults for all deployments (database: global_config table)\nChart-specific config - Per-chart defaults (database: global_config table)\nService YAML config - Service-specific overrides (highest priority)\nService-level configuration always takes precedence over global defaults.","global-configuration-database#Global Configuration (Database)":"Global configurations are stored in the global_config table in the database. Each configuration is stored as a row with:\nkey: The configuration name (e.g., 'helmDefaults', 'postgresql', 'redis')\nconfig: JSON object containing the configuration","helmdefaults-configuration#helmDefaults Configuration":"Stored in database with key helmDefaults:\nField Descriptions:\nenabled: When true, enables native Helm deployment for all services unless they explicitly set deploymentMethod: \"ci\"\ndefaultArgs: Arguments automatically appended to every Helm command (appears before service-specific args)\ndefaultHelmVersion: The Helm version to use when not specified at the service or chart level","chart-specific-configuration#Chart-specific Configuration":"Example: PostgreSQL configuration stored with key postgresql:\nThese global configurations are managed by administrators and stored in the\ndatabase. They provide consistent defaults across all environments and can be\noverridden at the service level.","usage-examples#Usage Examples":"","quick-experiment-deploy-jenkins#Quick Experiment: Deploy Jenkins!":"Want to see native Helm in action? Let's deploy everyone's favorite CI/CD tool - Jenkins! This example shows how easy it is to deploy popular applications using native Helm.\nüéâ That's it! With just a few lines of configuration, you'll have Jenkins\nrunning in your Kubernetes cluster.\nTo access your Jenkins instance:\nCheck the deployment status in your PR comment\nClick the Deploy Logs link to monitor the deployment\nOnce deployed, Jenkins will be available at the internal hostname\nFor more Jenkins configuration options and values, check out the Bitnami\nJenkins chart\ndocumentation.\nThis same pattern works for any Bitnami chart (PostgreSQL, Redis, MongoDB) or\nany other public Helm chart!","basic-service-deployment#Basic Service Deployment":"","postgresql-with-overrides#PostgreSQL with Overrides":"","custom-environment-variables#Custom Environment Variables":"Lifecycle supports flexible environment variable formatting through the envMapping configuration. This feature allows you to control how environment variables from your service configuration are passed to your Helm chart.\nWhy envMapping? Different Helm charts expect environment variables in\ndifferent formats. Some expect an array of objects with name and value\nfields (Kubernetes standard), while others expect a simple key-value map. The\nenvMapping feature lets you adapt to your chart's requirements.","default-envmapping-configuration#Default envMapping Configuration":"You can define default envMapping configurations in the global_config database table. These defaults apply to all services using that chart unless overridden at the service level.Example: Setting defaults for your organization's chart\nWith this configuration, any service using the myorg-web-app chart will automatically use array format for environment variables:\nSetting envMapping in global_config is particularly useful when: - You have\na standard organizational chart used by many services - You want consistent\nenvironment variable handling across services - You're migrating multiple\nservices and want to reduce configuration duplication","array-format#Array Format":"Best for charts that expect Kubernetes-style env arrays.\nThis produces the following Helm values:\nYour chart's values.yaml would use it like:","map-format#Map Format":"Best for charts that expect a simple key-value object.\nThis produces the following Helm values:\nNote: Underscores in environment variable names are converted to double\nunderscores (__) in map format to avoid Helm parsing issues.\nYour chart's values.yaml would use it like:","complete-example-with-multiple-services#Complete Example with Multiple Services":"","templated-variables#Templated Variables":"Lifecycle supports template variables in Helm values that are resolved at deployment time. These variables allow you to reference dynamic values like build UUIDs, docker tags, and internal hostnames.","available-variables#Available Variables":"Template variables use the format {{{variableName}}} and are replaced with actual values during deployment:\nVariable\tDescription\tExample Value\t{{{serviceName_dockerTag}}}\tDocker tag for a service\tmain-abc123\t{{{serviceName_dockerImage}}}\tFull docker image path\tregistry.com/org/repo:main-abc123\t{{{serviceName_internalHostname}}}\tInternal service hostname\tapi-service.env-uuid.svc.cluster.local\t{{{build.uuid}}}\tBuild UUID\tenv-12345\t{{{build.namespace}}}\tKubernetes namespace\tenv-12345","usage-in-values#Usage in Values":"Docker Image Mapping: When using custom charts, you'll need to map {{{serviceName_dockerImage}}} or {{{serviceName_dockerTag}}} to your chart's expected value path. Common patterns include:\nimage.repository and image.tag (most common)\ndeployment.image (single image string)\napp.image or application.image\nCustom paths specific to your chart\nCheck your chart's values.yaml to determine the correct path.","image-mapping-examples#Image Mapping Examples":"Important: Always use triple braces {{{variable}}} instead of double braces {{variable}} for Lifecycle template variables. This prevents Helm from trying to process them as Helm template functions and ensures they are passed through correctly for Lifecycle to resolve.","template-resolution-order#Template Resolution Order":"Lifecycle resolves {{{variables}}} before passing values to Helm\nThe resolved values are then passed to Helm using --set flags\nHelm processes its own template functions (if any) after receiving the resolved values","example-with-service-dependencies#Example with Service Dependencies":"","deployment-process#Deployment Process":"Job Creation: A Kubernetes job is created in the ephemeral namespace 2.\nRBAC Setup: Service account with namespace-scoped permissions is created\nGit Clone: Init container clones the repository 4. Helm Deploy:\nMain container executes the Helm deployment 5. Monitoring: Logs are\nstreamed in real-time via WebSocket","concurrent-deployment-handling#Concurrent Deployment Handling":"Native Helm automatically handles concurrent deployments by:\nDetecting existing deployment jobs\nForce-deleting the old job\nStarting the new deployment\nThis ensures the newest deployment always takes precedence.","monitoring-deployments#Monitoring Deployments":"","deploy-logs-access#Deploy Logs Access":"For services using native Helm deployment, you can access deployment logs through the Lifecycle PR comment:\nAdd the lifecycle-status-comments! label to your PR\nIn the status comment that appears, you'll see a Deploy Logs link for each service using native Helm\nClick the link to view real-time deployment logs","log-contents#Log Contents":"The deployment logs show:\nGit repository cloning progress (clone-repo container)\nHelm deployment execution (helm-deploy container)\nReal-time streaming of all deployment output\nSuccess or failure status","chart-types#Chart Types":"Lifecycle automatically detects and handles three chart types:\nType\tDetection\tFeatures\tORG_CHART\tMatches orgChartName AND has helm.docker\tDocker image injection, env var transformation\tLOCAL\tName is \"local\" or starts with \"./\" or \"../\"\tFlexible envMapping support\tPUBLIC\tEverything else\tStandard labels and tolerations\t\nThe orgChartName is configured in the database's global_config table with\nkey orgChart. This allows organizations to define their standard internal\nHelm chart.","troubleshooting#Troubleshooting":"","deployment-fails-with-another-operation-in-progress#Deployment Fails with \"Another Operation in Progress\"":"Symptom: Helm reports an existing operation is blocking deploymentSolution: Native Helm automatically handles this by killing existing jobs. If the issue persists:","environment-variables-not-working#Environment Variables Not Working":"Symptom: Environment variables not passed to the deploymentCommon Issues:\nenvMapping placed under chart instead of directly under helm\nIncorrect format specification (array vs map)\nMissing path configuration\nCorrect Configuration:","migration-example#Migration Example":"Here's a complete example showing how to migrate from GitHub-type services to Helm-type services:","before-github-type-services#Before: GitHub-type Services":"","after-helm-type-services-with-native-deployment#After: Helm-type Services with Native Deployment":"","key-migration-points#Key Migration Points":"Service Type Change: Changed from github: to helm: configuration\nRepository Location: repository and branchName move from under github: to directly under helm:\nDeployment Method: Added deploymentMethod: \"native\" to enable native Helm\nChart Configuration: Added chart: section with local or public charts\nEnvironment Mapping: Added envMapping: to control how environment variables are passed\nHelm Arguments: Added args: for Helm command customization\nDocker Configuration: Kept existing docker: config for build process\nNote that when converting from GitHub-type to Helm-type services, the\nrepository and branchName fields move from being nested under github: to\nbeing directly under helm:.\nMany configuration options (like Helm version, args, and chart details) can be\ndefined in the global_config database table, making the service YAML\ncleaner. Only override when needed."}},"/docs/features/service-dependencies":{"title":"Service Dependencies","data":{"":"This document will cover environment.{defaultServices,optionalServices} and service.requires, their differences, impact scope, and usage.","environmentdefaultservicesoptionalservices#environment.{defaultServices,optionalServices}":"","impact-scope#Impact scope":"Scope\tImpact\tService repo*\t‚úÖ\tOutside repo*\t‚ùå\tdev-0*\t‚ùå\t\nThis represents the default environment that will be created by lifecycle when a pull request is opened in the service repo* and does not have any impact on outside repos, dev-0, or any other static environments that use this service.","servicesrequires#services.requires":"","impact-scope-1#Impact scope":"Scope\tImpact\tService repo*\t‚úÖ\tOutside repo*\t‚úÖ\tdev-0*\t‚úÖ\t\nservices.requires has an impact across the board; hence, it is important to understand how it works and when we should use them.Please read the info blocks below carefully.You can think of services.requires as a hard dependency definition. For example, if you have an API service and a database, the API service will have a hard dependency on the database.\nIn this scenario, the database should not be defined as the default service. Instead, we should make the dependency explicitly clear by adding the database to the API‚Äôs requires block.\nBy doing this, we ensure that any outside repo that wants to use our API service will get the database along with it but only needs to specify the API service in their defaultServices or optionalServices.\nOnly services defined in lifecycle.yaml should be used in the requires\narray. If a service is defined in an outside repo, use\nenvironment.defaultServices instead.\nDo not use services in the services.requires if the service itself is not\ndefined in the same lifecycle.yaml.\nServices defined in the requires block will only be resolved 1 level down.\nThis is a very important nuance, which we get tripped by regularly.","examples#Examples":"To better illustrate the above statement, consider this example.Repository A r-A has 3 services s-A, s-B, and s-C.\ns-A requires s-B.\ns-B requires s-C.\nAs you can see, s-A has an indirect dependency on s-C through s-B.","scenario-1-pull-request-in-service-repo-#Scenario 1: Pull Request in Service repo* ‚úÖ":"When we open a pull request in r-A repo, lifecycle will deploy 3 services: s-A, s-B, and s-C.","breakdown#Breakdown":"Lifecycle deploys s-A and s-B because they are defined in defaultServices.\nServices defined in the requires block will only be resolved one level down.\nOnly services defined in lifecycle.yaml should be used in the requires array. If a service is defined in an outside repo, use environment.defaultServices instead.","scenario-2-#Scenario 2: ‚ùå":"Repository B r-B has service s-X and also defines an outside repo r-A service s-A as environment.defaultServices.","breakdown-1#Breakdown":"Lifecycle deploys s-X and s-A because they are defined in defaultServices.\nLifecycle deploys s-B because it is a 1st level dependency of a service (s-A) listed in defaultServices.\nLifecycle does not deploy s-C since it is not a 1st level dependency of any service listed in defaultServices or optionalServices.\nThe way this scenario manifests is lifecycle will deploy s-X, s-A, and s-B, but the build will likely fail because s-B is missing a required dependency s-C.","solutions#Solutions":"There are 2 ways to address this depending on your use case.","solution-1#Solution 1":"Add s-B to r-B‚Äôs environment.defaultServices block in r-B.lifecycle.yaml. In effect, this will make s-C a first-level dependency.","solution-2#Solution 2":"Add s-C to the services.requires block of r-A in r-A.lifecycle.yaml. This will also make s-C a first-level dependency.","choosing-the-right-solution#Choosing the Right Solution":"In summary, the solution you should use depends on how you want your service to be consumed in an outside repo*.\nIf you want outside repos to explicitly include s-A and s-B, use Solution 1.\nIf you want outside repos to only include s-A and let dependencies resolve automatically, use Solution 2.","terminology#Terminology":"Service repo: The repository where lifecycle.yaml is defined.\nOutside repo: Another repository referencing it.\ndev-0: Default static environment."}},"/docs/features/template-variables":{"title":"Template Variables","data":{"overview#Overview":"Lifecycle uses Mustache as the template rendering engine.","available-template-variables#Available Template Variables":"The following template variables are available for use within your configuration. Variables related to specific services should use the service name as a prefix.","general-variables#General Variables":"{{{buildUUID}}} - The unique identifier for the Lifecycle environment, e.g., lively-down-881123.\n{{{namespace}}} - Namespace for the deployments, e.g., env-lively-down-881123.\n{{{pullRequestNumber}}} - The GitHub pull request number associated with the environment.","service-specific-variables#Service-Specific Variables":"For service-specific variables, replace <service_name> with the actual service name.\n{{{<service_name>_internalHostname}}} - The internal hostname of the deployed service. If the service is optional and not deployed, it falls back to defaultInternalHostname.\nservice_internalHostname will be substituted with local cluster full\ndomain name like service.namespace.svc.cluster.local to be able to work\nwith deployments across namespaces.\n{{{<service_name>_publicUrl}}} - The public URL of the deployed service. If optional and not deployed, it defaults to defaultPublicUrl under the services table.\n{{{<service_name>_sha}}} - The GitHub SHA that triggered the Lifecycle build.\n{{{<service_name>_branchName}}} - The branch name of the pull request that deployed the environment.\n{{{<service_name>_UUID}}} - The build UUID of the service. If listed under optionalServices or defaultServices, its value depends on whether the service is selected:\nIf selected, it is equal to buildUUID.\nIf not selected (or if service not part of deploys created), it defaults to dev-0.","usage-example#Usage Example":"This ensures the PUBLIC_URL and INTERNAL_HOST variables are dynamically assigned based on the ephemeral environment deployment.\nUndefined variables will result in an empty string unless handled explicitly.\nUse triple curly braces ({{{ }}}) to prevent unwanted HTML escaping.\nEnsure service names are correctly referenced in the template without any spaces.\nFor more details, refer to the Mustache.js documentation."}},"/docs/features/webhooks":{"title":"Webhooks","data":{"":"Lifecycle can invoke third-party services when a build state changes.Webhooks allow users to automate external processes such as running tests, performing cleanup tasks, or sending notifications based on environment build states.","supported-types#Supported Types":"Lifecycle supports three types of webhooks:\ncodefresh - Trigger Codefresh pipelines\ndocker - Execute Docker images as Kubernetes jobs\ncommand - Run shell commands in a specified Docker image","common-use-cases#Common Use Cases":"When a build status is deployed, trigger end-to-end tests.\nWhen a build status is error, trigger infrastructure cleanup or alert the team.\nRun security scans on built containers.\nExecute database migrations after deployment.\nSend notifications to Slack, Discord, or other communication channels.\nPerform smoke tests using custom test containers.","configuration#Configuration":"Webhooks are defined in the lifecycle.yaml under the environment.webhooks section.Below is an example configuration for triggering end-to-end tests when the deployed state is reached.","examples#Examples":"","codefresh#codefresh":"The codefresh type triggers existing Codefresh pipelines when build states change.\nstate: deployed ‚Üí Triggers the webhook when the build reaches the deployed state.\ntype: codefresh ‚Üí Specifies that this webhook triggers a Codefresh pipeline.\nname ‚Üí A human-readable name for the webhook.\npipelineId ‚Üí The unique Codefresh pipeline ID.\ntrigger ‚Üí Codefresh pipeline's trigger to execute.\nenv ‚Üí Passes relevant environment variables (e.g., branch and TEST_URL).\nstate: error ‚Üí Triggers the webhook when the build fails.\ntype: codefresh ‚Üí Invokes a Codefresh cleanup pipeline.\ntrigger: cleanup ‚Üí Codefresh pipeline's trigger to execute.\nenv ‚Üí Includes necessary variables, such as branch and CLEANUP_TARGET.","docker#docker":"The docker type allows you to execute any Docker image as a Kubernetes job when build states change.\nDocker webhooks run as Kubernetes jobs in the same namespace as your build.\nThey have a default timeout of 30 minutes and resource limits of 200m CPU and\n1Gi memory.\ndocker.image ‚Üí Docker image to execute (required)\ndocker.command ‚Üí Override the default entrypoint (optional)\ndocker.args ‚Üí Arguments to pass to the command (optional)\ndocker.timeout ‚Üí Maximum execution time in seconds (optional, default: 1800)","command#command":"The command type is a simplified version of Docker webhooks, ideal for running shell scripts or simple commands.\nMake sure to replace placeholder values like webhook URLs and pipeline IDs\nwith your actual values.\ncommand.image ‚Üí Docker image to run the script in (required)\ncommand.script ‚Üí Shell script to execute (required)\ncommand.timeout ‚Üí Maximum execution time in seconds (optional, default: 1800)","trigger-states#Trigger states":"Webhooks can be triggered on the following build states:\ndeployed ‚Üí Service successfully deployed and running\nerror ‚Üí Build or deployment failed\ntorn_down ‚Üí Environment has been destroyed","note#Note":"All webhooks for the same state are executed serially in the order defined.\nWebhook failures do not affect the build status.\nWebhook invocations can be viewed at /builds/[uuid]/webhooks page(latest 20 invocations). Use the API to view all invocations.\ndocker and command type's logs are not streamed when the job is still in progress and are available only after the job completes."}},"/docs/getting-started/configure-environment":{"title":"Configure environment","data":{"":"Now that we've created and deployed our first Lifecycle environment, let's learn how to customize it by configuring services and dependencies.","understanding-configuration#Understanding Configuration":"First, let's take a look at the lifecycle.yaml configuration file at the root dir of lifecycle-examples repository:","default-and-optional-services#Default and Optional Services":"We have our dependencies defined in defaultServices and optionalServices:\ndefaultServices ‚Äì These services are always built and deployed with the environment. They form the core foundation of the environment and are required for it to function correctly.\noptionalServices ‚Äì These services can be built on demand, only when explicitly needed. If they are not selected during a PR, they default to using a static environment (e.g., dev-0).","template-variables#Template Variables":"Notice how there are template variables defined in service named frontend > github.docker.env:\nThis API_URL and CACHE_URL variables are dynamically templated by Lifecycle and provided during the build and deploy steps for the frontend service.\nRead more about supported template variables\nhere","static-environment-as-a-fallback#Static Environment as a Fallback":"Since cache is an optional service, this service defaulted to using a static environment(dev-0) as a fallback. This allows us to reuse existing environments instead of rebuilding everything from scratch when there are no changes.","check-template-variables#Check Template Variables":"To view how the fallback URL works,\nOpen your Tasks App(frontend) from the deployed environment.\nNavigate to the Variables page.\nSearch for _URL and check its value.\nIt should look like:\nNotice how CACHE_URL defaults to the dev-0(static) environment for the optional cache.","configuring-services#Configuring Services":"Now, let's say you also want to the cache component to test, build and deploy it in your environment.","enable-cache-deployment#Enable Cache Deployment":"Navigate to the Lifecycle PR comment on GitHub.\nSelect the cache checkbox in the comment. That's it!\nLifecycle will now start building and deploying the cache service for your specific environment.\nWait for the build to complete. You can monitor the progress in the status comment.","confirm-the-new-cache-url#Confirm the New Cache URL":"Once the cache is deployed, go back to your frontend app‚Äôs Variables page.\nCheck the CACHE_URL value.\nIt should now look like:\nNow, you're running your cache from your own environment instead of an existing static deploy!\nCheck the application‚Äôs Tasks page while you‚Äôre here and observe the completely different data, as this environment uses a freshly built and seeded database.","build-flexible-environments#Build Flexible Environments":"With this approach, you can:\nBuild any combination of frontend and backend services.\nUse custom branches for different services.\nTest different versions of your app.\nCheck how to use Mission Control comments for configuring your environment\nhere\nThis gives you a custom, isolated testing environment that mirrors your\nproduction setup while allowing flexibility in development and validation.","summary#Summary":"Services marked as optional in lifecycle.yaml will default to static environments unless explicitly built.\nYou can enable/disable any service directly from the Lifecycle PR comment.\nLifecycle automates dependency management, ensuring your services deploy in the correct order.\nNow you're ready to customize your Lifecycle environments like a pro! üë©‚Äçüíª"}},"/docs/getting-started/create-environment":{"title":"Create environment","data":{"":"In this walk through, we will make a simple change to an example frontend repository and create our first ephemeral environment using Lifecycle.","1-fork-the-repository#1. Fork the Repository":"Fork the lifecycle-examples repository to your org or personal account and install your newly minted GitHub App to the forked repository.\nNavigate to https://github.com/settings/apps (for personal accounts) or https://github.com/organizations/<org>/settings/apps (for org accounts).\nFind the Lifecycle GitHub App and click on Edit.\nChoose Install App from sidebar and click the Settings  icon.\nSelect the forked repository from the list and select Save.","2-create-a-new-branch#2. Create a New Branch":"Clone the repo and create a branch named lfc-config:\nor if you are using GitHub Desktop, you can create a new branch from the UI.","3-update-lifecycle-configuration#3. Update Lifecycle Configuration":"Open the lifecycle.yaml file in the root of the repository and update the frontend service's repository to your github username or org.Before:\nAfter:","4-commit--push-your-changes#4. Commit & Push Your Changes":"","5-create-a-pull-request#5. Create a Pull Request":"Open a Pull Request (PR) from lfc-config to main in the forked repository.\nSubmit the PR.","6-lifecycle-pr-comment#6. Lifecycle PR Comment":"After submitting the PR, you‚Äôll see a GitHub comment from Lifecycle on your pull request.üîπ This PR comment is the mission control for your ephemeral environment. It provides:\nA status update of the build and deploy process.\nA list of services configured for the environment.\nA link to the Lifecycle UI where you can view logs, deployments, and environment details.\nIf there is no comment from Lifecycle, it means the app is not configured\ncorrectly or the GitHub App is not installed in the repository. Please refer\nto the Missing Comment page for\nmore information.","7-add-lifecycle-status-comments-label#7. Add lifecycle-status-comments! label":"The additional label lifecycle-status-comments! provides more detailed information about the environment status and links to access the running application.üîπ The comments provides insights into:\nBuild & Deploy Status: Track when your environment is ready.\nEnvironment URLs: Access the running frontend app.\nTelemetry Links: Links to telemetry, build and deploy logs. (if enabled)","8-wait-for-deployment#8. Wait for Deployment":"Wait for the builds & deploys to complete. Once the status updates to deployed, your environment is live! üöÄWhen a new commit is pushed to your pull request Lifecycle automatically builds and deploys again so you always have the latest version of the application.\nIf there are any errors during the build or deploy process, the environment\nwill not be created, and you will see an error message in the Lifecycle\ncomment.\nYou can check the logs from lifecycle-worker pods in your cluster to debug\nthe issue:  kubectl logs deploy/lifecycle-worker -n lifecycle-app -f","9-checkout-the-deployed-application#9. Checkout the deployed application":"Once the deployment is complete, you can access your environment at the URL provided in the Lifecycle comment on your pull request. Click on the frontend link to open your application in a new tab.The application has two simple pages:\n/tasks ‚Äì A simple to-do list.\n/variables ‚Äì Displays all environment variables from the container.","next-steps#Next Steps":"Now that your first ephemeral environment is ready, move on to the next section where we:üß™ Test the environment.\nüß≠ Explore the comments and logs.\n‚öôÔ∏è Customize the configuration."}},"/docs/getting-started/delete-environment":{"title":"Delete environment","data":{"":"To tear down an environment, you can do one of the following:\nMerge or close the pull request: This will automatically clean up the environment.\nApply the lifecycle-disabled! label: This will immediately trigger the environment deletion process.\nThe lifecycle-disabled! label is useful in scenarios where:\nThe environment infrastructure is experiencing issues.\nThe data within the environment is corrupt.\nYou need to restart or rebuild the environment from scratch without waiting for a PR to be merged or closed.\nSimply apply the label to the PR associated with the environment, and Lifecycle will automatically tear it down.\nRead more about how pull request labels control auto deploy in repositories\nhere\nUsing these methods, you can efficiently manage and clean up environments to ensure smooth development and testing workflows. üßπ"}},"/docs/getting-started/explore-environment":{"title":"Explore environment","data":{"":"Now that we've deployed our first Lifecycle environment, let‚Äôs take a tour of the PR comments to understand how to interact with our ephemeral environment.","test-your-application#Test Your Application":"Let's navigate to the deployed frontend app from the PR comment.\nClick on the frontend link in the PR comment to navigate to your deployed application.\nAdd a task and complete few tasks to update data in backend.\nNavigate to the variables page and checkout the variables in your application's container.\nThats it! You have successfully deployed and tested the best todo app in the world! üéâ","mission-control-comment#Mission Control Comment":"The Lifecycle PR comment in your pull request serves as the mission control for your ephemeral environment.","what-you-can-do-in-the-pr-comment#What You Can Do in the PR Comment":"Editable Checkboxes: Select or deselect services to include in your environment.\nRedeploy Checkbox: Triggers a redeploy (useful for transient issues).\nDeployment Section: Provides URLs to your deployed services.\nRead more about Mission Control comment\nhere","status-comment#Status Comment":"When we add the lifecycle-status-comments! label to our pull request, Lifecycle will automatically add a status comment to the PR.This comment provides real-time updates on the status, links to your deployments including the build progress and service statuses.\nNotice the following while the environment is being built:\nThe status comment is updated in real-time.\nThe status of each service is displayed.\nThe build logs are available for each service.","next-steps#Next Steps":"In the next section, we will:‚öôÔ∏è Customize our configuration\n‚òëÔ∏è Enable and build an optional service(cache) support your applicationReady to level up your ephemeral environment? Let's go! üèÉ‚Äç‚û°Ô∏è"}},"/docs/getting-started/explore-static-environment":{"title":"Explore static environment","data":{"":"A static environment in Lifecycle is a persistent environment that serves as a fallback when dependent services do not need to be rebuilt.Unlike ephemeral environments that are built on short lived pull requests, static environments are built on top of long lived pull requests. These environments exist continuously and update automatically as changes are merged into the default branch of configured services.","what-is-dev-0#What is dev-0":"The default static environment is dev-0. This environment ensures that there is always a stable and up-to-date version of services available without needing to build every dependency manually.\nThe dev-0 environment should be created for your installation.During the initial bootstrapping of Lifecycle, the dev-0 build record is created automatically but this itself does not have any services built.","create-dev-0#Create dev-0":"Delete the dummy dev-0 build record from builds table in the database\nCreate a repository named lifecycle-static-env in your GitHub account\nInstall the Lifecycle GitHub App in this repository\nCreate a pull request in this repository with branch dev-0\nAdd lifecycle.yaml file to the root of the repository with all the services you want to include in the dev-0 environmentExample:\nDeploy the dev-0 environment by adding lifecycle-deploy! label to the pull request\nUpdate uuid for the environment to dev-0 in the mission control comment\nFinally, execute this query to track default branches of the services in the dev-0 environment:","key-features#Key Features":"üèóÔ∏è Fallback for Optional Services\nWhen optional services are not explicitly built in an ephemeral environment, Lifecycle defaults to using the latest build from dev-0.\nüí™ Based on a Persistent PR\nSimilar to ephemeral environments, dev-0 is based on a PR, but it remains open and continuously updates.\nüë£ Tracks Changes on Default Branch Merges\nWhenever a service has a new change merged to its main branch, dev-0 will automatically pull, build, and redeploy the latest changes.\nThis ensures dev-0 always contains the freshest version of all services."}},"/docs/getting-started/terminology":{"title":"Terminology","data":{"":"This glossary provides an overview of key Lifecycle concepts and terminology. Let's see how they fit into the environment setup and deployment process.","repository#Repository":"A repository refers to a GitHub repository. Each environment that is built must have a default repository and an associated pull request.","service#Service":"A service is a deployable artifact. It can be a Docker container, CI pipeline, RDS database, or Helm chart. A single repository can contain multiple services.Example: \nfrontend-service and frontend-cache are two services required for the frontend application to function correctly.","environment#Environment":"An environment is a stack of services built and connected together.\ndefaultServices are built and deployed in an environment by default.\noptionalServices can be built and deployed only when needed; otherwise, they fallback to the default static environment.","static-environment#Static Environment":"A static environment is a long-lived environment based on a pull request. It tracks branches from configured services and updates automatically when new changes are merged.","build#Build":"A build is the actual instance of the process to build and deploy services within an environment.\nEach build is uniquely identified by Lifecycle using a UUID (e.g., arm-model-060825 or dev-0).\nA build contains one deploy per service in the configuration.","deploy#Deploy":"A deploy manages the build and deployment execution of a service within an environment.Example:\nIn a frontend environment, frontend-service and frontend-cache are two deploys created for the environment, each mapped to a unique build UUID.","webhook#Webhook":"Lifecycle can invoke third-party services when a build state changes. Currently, only Codefresh triggers are supported.","example#Example":"When the build status is deployed, trigger end-to-end tests.\nWhen the build status is error, trigger infrastructure cleanup."}},"/docs/schema/aurora-restore":{"title":"Aurora Restore Service","data":{"":"The auroraRestore service type restores AWS Aurora database snapshots for use in ephemeral environments. This allows you to test against production-like data.\nThis documentation is still in progress and will be updated shortly with the\nlatest information.","required-fields#Required Fields":"Field\tType\tDescription\tcommand\tstring\tCommand to execute for the restore operation\targuments\tstring\tArguments passed to the command","how-it-works#How It Works":"When the environment is deployed, Lifecycle executes the configured command with arguments\nThe command typically triggers an Aurora snapshot restore process\nOnce complete, the restored database is available for other services\nThe auroraRestore service type is a specialized service that runs a command\nto initiate database restoration. The actual restore logic is implemented in\nyour command/script.","example#Example":"","use-cases#Use Cases":"Production data testing - Test features against realistic production data\nDatabase migrations - Validate migrations against production schema\nPerformance testing - Benchmark against production-scale data\nBug reproduction - Reproduce issues with production data state\nWhen restoring from production snapshots, ensure sensitive data is properly\nanonymized or that access is appropriately restricted.","implementation-notes#Implementation Notes":"The restore command is executed as part of the environment deployment process. Your script should:\nHandle authentication with AWS\nInitiate the snapshot restore\nWait for the restore to complete\nConfigure network access to the restored instance\nReturn the connection information\nThe restored database endpoint can then be referenced by other services using template variables."}},"/docs/schema/codefresh":{"title":"Codefresh Service","data":{"":"The codefresh service type delegates deployment to external Codefresh pipelines. This is useful when you have existing CI/CD pipelines that handle complex build and deployment logic.","example#Example":"","required-fields#Required Fields":"Field\tType\tDescription\trepository\tstring\tGitHub repository in owner/repo format\tbranchName\tstring\tBranch to build from\tdeploy.pipelineId\tstring\tCodefresh pipeline ID for deployment\tdeploy.trigger\tstring\tPipeline trigger name for deployment\tdestroy.pipelineId\tstring\tCodefresh pipeline ID for destruction\tdestroy.trigger\tstring\tPipeline trigger name for destruction","optional-fields#Optional Fields":"Field\tType\tDescription\tenv\tobject\tEnvironment variables passed to pipelines","pipeline-configuration#Pipeline Configuration":"","deploy#deploy":"Configure the pipeline triggered when deploying the service.","destroy#destroy":"Configure the pipeline triggered when tearing down the environment.","environment-variables#Environment Variables":"Pass environment variables to both deploy and destroy pipelines:\nEnvironment variables defined in env are passed to both deploy and destroy\npipelines. Use template variables to reference other services.","how-it-works#How It Works":"When an environment is deployed, Lifecycle triggers the deploy pipeline with the configured environment variables.\nThe pipeline receives context about the deployment including branch, namespace, and service information.\nWhen the environment is torn down, Lifecycle triggers the destroy pipeline to clean up resources.","use-cases#Use Cases":"Complex build processes - When builds require multi-stage pipelines or custom tooling\nExternal dependencies - When deployment requires interaction with external systems\nExisting CI/CD - When you have established Codefresh pipelines you want to reuse\nCustom deployment logic - When standard Kubernetes deployment isn't sufficient\nCodefresh pipelines run asynchronously. Lifecycle tracks pipeline status but\nthe actual deployment logic is managed by Codefresh.","finding-pipeline-ids#Finding Pipeline IDs":"To find your Codefresh pipeline ID:\nOpen your pipeline in Codefresh\nThe pipeline ID is in the URL: https://g.codefresh.io/pipelines/edit/new/.../<pipeline-id>\nOr use the Codefresh CLI: codefresh get pipelines"}},"/docs/schema/configuration":{"title":"Configuration Service","data":{"":"The configuration service type creates a configuration-only service that doesn't deploy any containers. It's useful for managing feature flags, shared configuration, or service metadata that other services can reference.\nThis documentation is still in progress and will be updated shortly with the\nlatest information.","required-fields#Required Fields":"Field\tType\tDescription\tdefaultTag\tstring\tConfiguration version tag\tbranchName\tstring\tBranch name for configuration source","how-it-works#How It Works":"A configuration service:\nDoes not deploy any containers or pods\nRegisters a service entry in Lifecycle for reference by other services\nProvides a way to version and track configuration changes\nCan be referenced by other services using template variables","example#Example":"","use-cases#Use Cases":"","feature-flags#Feature Flags":"Track feature flag configuration versions:","shared-configuration#Shared Configuration":"Reference shared configuration across services:","environment-metadata#Environment Metadata":"Track environment-specific metadata:\nConfiguration services are lightweight entries in Lifecycle's service\nregistry. They don't consume cluster resources but allow other services to\nreference configuration state."}},"/docs/schema/docker":{"title":"Docker Service","data":{"":"The docker service type deploys pre-built Docker images without building from source. This is ideal for databases, caches, message queues, and other infrastructure components.","examples#Examples":"PostgreSQL database (internal only):\nReference from other services:\nRedis cache:\nReference from other services:\nMySQL with persistent storage:\nComplete configuration with all options:","fields-reference#Fields Reference":"","required-fields#Required Fields":"Field\tType\tDescription\tdockerImage\tstring\tDocker image name (e.g., postgres, redis)\tdefaultTag\tstring\tImage tag (e.g., 15-alpine, 7-alpine)","optional-fields#Optional Fields":"Field\tType\tDescription\tcommand\tstring\tOverride container entrypoint\targuments\tstring\tArguments passed to the command. Use %%SPLIT%% for spaces\tenv\tobject\tEnvironment variables\tports\tarray\tExposed container ports\tdeployment\tobject\tDeployment configuration (see below)\tenvLens\tboolean\tEnable environment lens ingress banner. Overrides the global features.envLens default.","deployment-options#Deployment Options":"The deployment section configures how the service is deployed to Kubernetes. All fields are optional.See GitHub Service - Deployment Options for detailed documentation of each field.\nField\tDescription\tpublic\ttrue = creates ingress, false = internal only\tcapacityType\tNode type: spot or on-demand\tresource\tCPU and memory requests/limits\treadiness\tHealth check using httpGet or tcpSocketPort\thostnames\tCustom hostname config (auto-constructed from global_config if omitted)\tnetwork\tIP whitelist, port mapping, gRPC support\tserviceDisks\tPersistent volume mounts\t\nFor databases and caches, you typically want public: false to keep them\ninternal to the cluster.","common-images#Common Images":"","databases#Databases":"","caches--queues#Caches & Queues":""}},"/docs/schema/environment":{"title":"Environment Configuration","data":{"":"The environment section controls how Lifecycle deploys and manages your ephemeral environments. It defines which services are deployed by default, which are optional, and how automation (webhooks) should behave.","fields-reference#Fields Reference":"Field\tType\tRequired\tDescription\tautoDeploy\tboolean\tNo\tAutomatically deploy on PR updates\tdefaultServices\tarray\tYes\tServices deployed with every environment\toptionalServices\tarray\tNo\tServices available on-demand\twebhooks\tarray\tNo\tAutomation triggers on state changes\tenabledFeatures\tarray\tNo\tFeature flags for the environment\tgithubDeployments\tboolean\tNo\tCreate GitHub deployment records\tuseGithubStatusComment\tboolean\tNo\tPost status comments on PRs","defaultservices#defaultServices":"Services listed in defaultServices are deployed automatically when an environment is created. At least one default service is required.","service-reference-fields#Service Reference Fields":"Each entry in defaultServices can include optional fields to override the service configuration:\nField\tType\tDescription\tname\tstring\tRequired. Name matching a service in the services array\trepository\tstring\tOverride the repository for this service\tbranch\tstring\tOverride the branch for this service\tserviceId\tnumber\tInternal service ID reference","optionalservices#optionalServices":"Services listed in optionalServices are not deployed by default but can be added to an environment on-demand through the Lifecycle UI or API.\nOptional services are useful for components that aren't always needed, like\nbackground workers, caching layers, or additional databases for specific\ntesting scenarios.","autodeploy#autoDeploy":"When autoDeploy is enabled, Lifecycle automatically triggers a deployment whenever the PR is updated (new commits pushed).\nWith autoDeploy: true, environments are created automatically on every\ncommit to the PR branch without requiring manual label addition.","webhooks#webhooks":"Webhooks allow you to trigger external actions when the environment reaches certain states. See the Webhooks page for detailed configuration.","examples#Examples":"The minimal configuration requires only defaultServices:\nConfiguration with both default and optional services:\nComplete configuration with all options:"}},"/docs/schema/github":{"title":"GitHub Service","data":{"":"The github service type builds and deploys applications from a GitHub repository using a Dockerfile. This is the most common service type for application code that needs to be built from source.","examples#Examples":"Minimal configuration with only required fields:\nConfiguration with deployment options:\nComplete configuration with all options:","docker-configuration#Docker Configuration":"The docker section defines how the application is built and run.","dockerdefaulttag#docker.defaultTag":"The default Docker image tag, typically matching the branch name.","dockerbuilder#docker.builder":"Configuration for the Docker build process. The engine field specifies which build engine to use:\nbuildkit - BuildKit engine (default)\ncodefresh - Codefresh build engine\nkaniko - Kaniko build engine","dockerapp-required#docker.app (Required)":"Configuration for the main application container:\ndockerfilePath - Required. Path to Dockerfile relative to repo root\ncommand - Override container entrypoint\narguments - Arguments passed to the command. Use %%SPLIT%% as a delimiter for spaces (e.g., -c%%SPLIT%%npm run start)\nenv - Environment variables\nports - Exposed container ports","dockerinit-optional#docker.init (Optional)":"Configuration for an init container that runs before the main application. Uses the same fields as docker.app.","deployment-options#Deployment Options":"The deployment section configures how the service is deployed to Kubernetes. All fields are optional.","public#public":"Controls whether the service is exposed via ingress for external access.\ntrue - Creates an ingress, service gets a public URL\nfalse - Internal only, accessible only within the cluster","capacitytype#capacityType":"Specifies the node capacity type for scheduling:\nspot - Use spot/preemptible instances (cost-effective)\non-demand - Use on-demand instances (more reliable)","resource#resource":"CPU and memory requests and limits for the container:\nField\tDescription\tcpu.request\tMinimum CPU guaranteed (e.g., 100m)\tcpu.limit\tMaximum CPU allowed (e.g., 1000m)\tmemory.request\tMinimum memory guaranteed (e.g., 256Mi)\tmemory.limit\tMaximum memory allowed (e.g., 1Gi)","readiness#readiness":"Health check configuration to determine when the service is ready to receive traffic.HTTP health check:\nField\tDescription\thttpGet.path\tHTTP endpoint path (e.g., /health)\thttpGet.port\tPort to check\t\nTCP health check:\nField\tDescription\ttcpSocketPort\tTCP port to check connectivity\t\nCommon fields:\nField\tDescription\tinitialDelaySeconds\tDelay before first check\tperiodSeconds\tInterval between checks\ttimeoutSeconds\tTimeout for each check\tsuccessThreshold\tConsecutive successes to be healthy\tfailureThreshold\tConsecutive failures to be unhealthy","hostnames#hostnames":"Custom hostname configuration. If omitted, hostnames are auto-constructed from global_config values.\nField\tDescription\thost\tCustom hostname suffix\tdefaultInternalHostname\tInternal Kubernetes hostname\tdefaultPublicUrl\tDefault public URL\tacmARN\tAWS ACM certificate ARN for TLS","network#network":"Advanced network configuration:\nField\tDescription\tipWhitelist\tArray of allowed IP ranges (CIDR notation)\tpathPortMapping\tMap URL paths to container ports\thostPortMapping\tMap hostnames to container ports\tgrpc.enable\tEnable gRPC support\tgrpc.host\tgRPC hostname","servicedisks#serviceDisks":"Persistent volume mounts for stateful data:\nField\tRequired\tDescription\tname\tYes\tVolume name\tmountPath\tYes\tPath inside the container\tstorageSize\tYes\tStorage size (e.g., 10Gi)\taccessModes\tNo\tReadWriteOnce or ReadWriteMany\tmedium\tNo\tStorage medium","fields-reference#Fields Reference":"","required-fields#Required Fields":"Field\tType\tDescription\trepository\tstring\tGitHub repository in owner/repo format\tbranchName\tstring\tBranch to build from\tdocker\tobject\tDocker build configuration (see above)","optional-fields#Optional Fields":"Field\tType\tDescription\tdeployment\tobject\tDeployment configuration (see above)\tenvLens\tboolean\tEnable environment lens ingress banner. Overrides the global features.envLens default.","template-variables#Template Variables":"Reference other services in your configuration using template variables. See the Template Variables guide for the complete list."}},"/docs/schema/overview":{"title":"Schema Overview","data":{"":"The lifecycle.yaml file is the core configuration file that defines how Lifecycle manages your ephemeral environments. Place this file at the root of your repository.","file-structure#File Structure":"A lifecycle.yaml file has three main sections:\nSection\tDescription\tversion\tSchema version (currently \"1.0.0\")\tenvironment\tControls deployment behavior, service grouping, and webhooks\tservices\tArray of service definitions with their configurations","service-types#Service Types":"Each service in the services array must have exactly one service type configured. Choose based on your deployment needs:\nService Type\tUse Case\tgithub\tBuild and deploy from a GitHub repository with Dockerfile\tdocker\tDeploy pre-built Docker images (databases, caches, etc.)\thelm\tDeploy using Helm charts (local or remote)\tcodefresh\tTrigger external Codefresh pipelines for deployment\tauroraRestore\tRestore AWS Aurora database snapshots\tconfiguration\tDeploy configuration-only services (feature flags, shared config)","choosing-a-service-type#Choosing a Service Type":"","minimal-example#Minimal Example":"Here's the simplest valid lifecycle.yaml for a single service:","complete-example#Complete Example":"A more complete example with multiple service types:","template-variables#Template Variables":"Throughout your configuration, you can use template variables like {{api_publicUrl}} or {{database_internalHostname}} to reference dynamic values from other services.See the Template Variables guide for the complete list of available variables and usage examples.","next-steps#Next Steps":"Environment Configuration - Configure deployment behavior and service grouping\nWebhooks - Automate actions on deployment events\nGitHub Service - Build and deploy from source\nDocker Service - Deploy pre-built images\nHelm Service - Deploy with Helm charts"}},"/docs/schema/helm":{"title":"Helm Service","data":{"":"The helm service type deploys applications using Helm charts. It supports local charts, OCI registries, and public Helm repositories. You can optionally include a Docker build step for custom images.\nFor advanced Helm deployment features like native Helm and environment\nvariable mapping, see the Native Helm\nDeployment guide.","examples#Examples":"Local chart with Docker build:\nOCI registry chart with Docker build:\nFor Helm template variables that should be resolved at deployment time, use triple braces {{{variable}}} to prevent Helm from processing them prematurely.\nPublic Bitnami chart (no Docker build):\nOther popular Bitnami charts:\nComplete configuration with all options:","chart-configuration#Chart Configuration":"The chart section is required and defines which Helm chart to deploy.\nCommonly used charts can be configured in the global_config table to reuse\nchart configuration across multiple services.","chartname-required#chart.name (Required)":"The chart name can be one of:\nValue\tDescription\t\"./\" or \"../\" prefix\tRelative path to local chart\t\"oci://...\"\tOCI registry chart URL\tChart name\tPublic chart name from a repository","chartrepourl#chart.repoUrl":"URL of the Helm repository (required for public charts).","chartversion#chart.version":"Specific chart version to deploy.","chartvalues#chart.values":"Array of Helm values in key=value format.","chartvaluefiles#chart.valueFiles":"Array of value file paths relative to the repository root.","docker-configuration#Docker Configuration":"The optional docker section defines how to build a custom image. This uses the same configuration as the GitHub service docker section.","dockerdefaulttag#docker.defaultTag":"The default Docker image tag, typically matching the branch name.","dockerbuilder#docker.builder":"Configuration for the Docker build process. The engine field specifies which build engine to use:\nbuildkit - BuildKit engine (default)\ncodefresh - Codefresh build engine\nkaniko - Kaniko build engine","dockerapp-required-when-using-docker#docker.app (Required when using docker)":"Configuration for the main application container:\ndockerfilePath - Required. Path to Dockerfile relative to repo root\ncommand - Override container entrypoint\narguments - Arguments passed to the command. Use %%SPLIT%% as a delimiter for spaces (e.g., -c%%SPLIT%%npm run start)\nenv - Environment variables\nports - Exposed container ports","dockerinit-optional#docker.init (Optional)":"Configuration for an init container that runs before the main application. Uses the same fields as docker.app.","fields-reference#Fields Reference":"Field\tType\tRequired\tDescription\tchart.name\tstring\tYes\tChart name or path\tchart.repoUrl\tstring\tFor public\tHelm repository URL\tchart.version\tstring\tNo\tChart version\tchart.values\tarray\tNo\tInline Helm values\tchart.valueFiles\tarray\tNo\tValue file paths\trepository\tstring\tFor builds\tGitHub repository\tbranchName\tstring\tFor builds\tBranch to build from\tdocker\tobject\tNo\tDocker build config\targs\tstring\tNo\tAdditional Helm arguments\tversion\tstring\tNo\tHelm CLI version\tdeploymentMethod\tstring\tNo\t\"native\" or \"ci\"\tenvLens\tboolean\tNo\tEnable environment lens ingress banner. Overrides the global features.envLens default.","templated-variables#Templated Variables":"Use templated variables in chart values to reference dynamic deployment values. See the Template Variables guide for the complete list.\nUse triple braces {{{variable}}} for Lifecycle template variables. This prevents Helm from trying to process them as Helm template functions.","native-helm-deployment#Native Helm Deployment":"For more control over Helm deployments, enable native Helm:\nNative Helm provides:\nDirect Kubernetes job execution\nReal-time deployment logs\nBetter handling of concurrent deployments\nFull Helm argument control\nSee the Native Helm Deployment guide for details."}},"/docs/schema/webhooks":{"title":"Webhooks Configuration","data":{"":"Webhooks allow you to trigger automated actions when an environment reaches certain states. They are configured under environment.webhooks in your lifecycle.yaml.\nFor detailed use cases, examples, and execution behavior, see the\nWebhooks guide.","common-fields#Common Fields":"All webhook types share these common fields:\nField\tType\tRequired\tDescription\tname\tstring\tNo\tHuman-readable name for the webhook\tdescription\tstring\tNo\tDescription of what the webhook does\tstate\tstring\tYes\tTrigger state: deployed, error, or torn_down\ttype\tstring\tYes\tWebhook type: codefresh, docker, or command\tenv\tobject\tYes\tEnvironment variables passed to the webhook","trigger-states#Trigger States":"State\tDescription\tdeployed\tAll services successfully deployed and running\terror\tBuild or deployment failed\ttorn_down\tEnvironment has been destroyed","webhook-types#Webhook Types":"","codefresh#codefresh":"Triggers an existing Codefresh pipeline.\nField\tType\tRequired\tDescription\tpipelineId\tstring\tYes\tCodefresh pipeline ID\ttrigger\tstring\tNo\tPipeline trigger name","docker#docker":"Runs a Docker image as a Kubernetes job.\nField\tType\tRequired\tDescription\tdocker.image\tstring\tYes\tDocker image to execute\tdocker.command\tarray\tNo\tOverride entrypoint command\tdocker.args\tarray\tNo\tArguments to pass to the command\tdocker.timeout\tnumber\tNo\tMax execution time in seconds (default: 1800)","command#command":"Runs a shell script in a specified Docker image.\nField\tType\tRequired\tDescription\tcommand.image\tstring\tYes\tDocker image to run the script in\tcommand.script\tstring\tYes\tShell script to execute\tcommand.timeout\tnumber\tNo\tMax execution time in seconds (default: 1800)","template-variables#Template Variables":"You can use template variables like {{api_publicUrl}} in the env section to reference dynamic values from your services.See the Template Variables guide for the complete list of available variables."}},"/docs/setup/configure-lifecycle":{"title":"Additional Configuration","data":{"":"We are in the final step of the setup process.This step is Optional but highly recommended to ensure the default IP Whitelist is set for the environments created by the Lifecycle app. This will help in securing the environments and restricting access to only the specified IPs or CIDR blocks.","set-default-ip-whitelist#Set Default IP Whitelist":"Connect to the postgres database using the psql command line tool or any other database client.\nDatabase password was auto generated during the infra setup and can be found\nretrieved from the app-postgres secret in the lifecycle-app\nnamespace.\nRetrieve the database password:\nRun the following SQL commands to update the configuration:\nNote that the infra setup with the OpenTofu modules below will open your\ncluster to the world. \nüõ°Ô∏è Make sure to shield your cluster by implementing appropriate network policies\nand access controls after the initial setup.Replace the defaultIPWhiteList under global_config.serviceDefaults with your actual IP whitelist or CIDR block to restrict access to the deployed environments.","enable-environment-lens-globally#Enable Environment Lens Globally":"The environment lens banner can be enabled globally for all services via the features row in global_config. Individual services can still override this by setting envLens explicitly in their lifecycle.yaml.\nAfter running this, refresh the config cache to apply the change.","refresh-config-cache#Refresh config cache":"This will refresh the configuration cache and apply the changes you made to the database for the Lifecycle app.We are all set! üéâ And ready to create our first PR based ephemeral environment."}},"/docs/setup/install-lifecycle":{"title":"Install Lifecycle","data":{"":"Now that the infrastructure components are setup, let's install the lifecycle app and create a new Github app that will send events to the application to process and create ephemeral dev environments.\nMake sure you have updated the kube config to be able to helm install in the\ncluster you just created!\nFollow installation steps in lifecycle helm chart\nWait for the installation to complete and verify that the pods are running:\nOnce the pods are running, you can access the application at your configured domain (e.g. https://app.0env.com)\nJust like that, you have successfully installed Lifecycle and set up the necessary infrastructure to start creating ephemeral environments for your GitHub pull requests!If you notice any secure certificate issues when accessing the application, you can check the status of your certificate using the following command:\nMake sure the certificate is in the Ready state. If it is not, you may need to wait a bit longer for the certificate to be issued or troubleshoot any issues with your DNS settings.Let's move on to the next step where we will create a GitHub app to connect Lifecycle with your repositories."}},"/docs/setup/create-github-app":{"title":"Configure Application","data":{"configure-buildkit-endpoint#Configure BuildKit Endpoint":"Before creating the GitHub app, you need to configure the BuildKit endpoint in the database:\nSet the HELM_RELEASE environment variable to your actual Helm release name\nbefore running the commands below.\nThe following commands will create the buildkit object and endpoint\nconfiguration if they don't exist, or update them if they do.","option-1-using-kubectl-exec-with-psql#Option 1: Using kubectl exec with psql":"Execute the following commands to connect to the PostgreSQL pod and run the query:","option-2-direct-sql-query#Option 2: Direct SQL query":"If you have direct database access, run the following SQL query (replace <YOUR-HELM-RELEASE> with your actual Helm release name):","refresh-configuration-cache#Refresh Configuration Cache":"After running either option above, refresh the configuration cache:\nReplace <your_domain> with your actual domain (e.g., 0env.com).","create-github-app#Create GitHub App":"To create a Github app that will send events to the Lifecycle with necessary permissions, follow these steps:\nMake sure you have admin access to the Github organization or account where\nyou want to create the app.\nNavigate to your installed Lifecycle app at https://app.<your_domain>/setup (replace <your_domain> with your actual domain. e.g. https://app.0env.com/setup).\nSelect Personal or Organization based on your needs.\nFill in the required fields:\nGithub App Name: A name for your app. (should be unique, use a prefix with your name or organization. Refer Github app naming convention here\nOrganization Name: Github organization name where the app will be created. Required if you selected Organization.\nClick Create App\nOn the Github app creation page, confirm the app name and click Create\nOnce the app is created, you will be redirected to the app installation page where you can choose one or more repositories to install the the newly minted app.\nMake sure to select the repositories you want the app to have access to. You\ncan always change this later in the app settings but adding atleast one\nrepository is required to proceed with the setup.\nVoila! üéâ Your Github app is now created and installed.\nClick Configure and Restart to apply the changes and start using the app.\nThe step above, sets up the global config values that Lifecycle app will use\ncreating ephemeral environments and processing pull requests. And restarts the\ndeployment for the github app secrets to take effect.\nLet's move on the final step where we will configure the Lifecycle app config for processing pull requests and creating ephemeral environments."}},"/docs/setup/prerequisites":{"title":"Prerequisites","data":{"":"Before we start with the setup, let's make sure the following prerequisites are in place:\nGitHub Account: You'll need either a personal or an organization GitHub account. Sign up for GitHub\nCloud Provider Account: A Google Kubernetes Engine (GKE) or Amazon Web Services (AWS) Account. You'll need an active account with either platform to proceed.\nSign up for Google Cloud and create a project\nSign up for AWS\nWe recommend using an isolated project or account in your cloud provider\nspecifically for this setup to begin with. This helps to keep your resources\norganized and manageable as you experiment with Lifecycle.\nCLI Tools\nOpenTofu ‚Äî Infrastructure as code tool (OpenTofu is a community-driven fork of Terraform).\nkubectl ‚Äî Command-line tool for interacting with Kubernetes clusters.\ngcloud or aws-cli ‚Äî Command-line tools for managing Google Cloud or AWS resources, respectively.\nCustom Domain: You will need a custom domain (e.g., 0env.com) to route traffic to your application environments. This is particularly important for setting up:\nPublic callback and webhook URLs for the GitHub App\nIngress routing within the Kubernetes cluster\nSecure (HTTPS) access via TLS certificates\nDNS Provider with Wildcard Support: The domain must be managed by a DNS provider that supports wildcard DNS records (e.g., *.0env.com). This is necessary to dynamically route traffic from GitHub to the Lifecycle app and to ephemeral environments.Supported DNS providers that support wildcard for infrastructure setup include:\nManual Setup:\nSetup a public DNS zone in Google Cloud to manage your domain's DNS records.\nFollow steps here to setup a\npublic DNS zone.\nWildcard DNS records will be created by the OpenTofu modules in the next steps.\nCLI Setup:\nUse the gcloud CLI to create a public DNS zone for your domain:\nUpdate your domain's DNS records with NS records provided by Google Cloud DNS. You can find these in the Google Cloud Console under the DNS zone you created.\nAWS Route 53: Amazon's scalable DNS web\nservice designed to route end users to Internet applications.Manual Setup:\nAuthenticate with AWS CLI using the role/usr you desire.\nEnsure you have your domain provisioned to accept wildcards; eg *.lifecycle.<your-domain>.com\nCLI Setup:\nIf you want to use Cloudflare as your primary DNS provider and manage your DNS records on Cloudflare, your domain should be using a full setup.\nThis means that you are using Cloudflare for your authoritative DNS nameservers.\nFollow the steps here to setup a public DNS zone in Cloudflare.\nEnsure that your domain‚Äôs nameservers are pointing to your chosen DNS provider\nat your registrar, and that you have permission to create and manage DNS\nrecords programmatically.  This is crucial for the setup to work\ncorrectly and will take time to propagate.Use https://dnschecker.org/#NS to verify that your domain's nameservers are correctly set up.\nOnce you have these prerequisites in place, you can proceed to the next steps in setting up the cluster and application."}},"/docs/setup/setup-infra":{"title":"Setup your cluster","data":{"":"Based on the prerequisites you've set up, you're now ready to configure your Kubernetes cluster for Lifecycle. This setup will ensure that your cluster is properly configured to run Lifecycle and manage your application environments effectively.\nNote that the infra setup with the OpenTofu modules below will open your\ncluster to the world. \nüõ°Ô∏è Make sure to shield your cluster by implementing appropriate network policies\nand access controls after the initial setup.\nClick on the cloud provider you are using to set up your cluster:\nGoogle Cloud Platform (GCP)\nAmazon Web Services (AWS)","google-cloud-platform#Google Cloud Platform":"","setup-application-credentials#Setup application credentials":"Enable Kubernetes Engine and Cloud DNS APIs:\nNote that you need to replace <PROJECT_ID> with your actual Google Cloud project ID not the project name.","bootstrap-infrastructure#Bootstrap infrastructure":"Clone the infrastructure repository:\nFollow steps in the infrastructure repository to set up the necessary infrastructure components.\nExample secrets.auto.tfvars file:\nInitialize and apply the Terraform configuration:\nThis will create the necessary infrastructure components, including the Kubernetes cluster, DNS records, database, redis and other resources required for Lifecycle to function properly.After the Terraform apply completes, you should have a fully functional Kubernetes cluster with the necessary resources set up.Let's test the public DNS setup by accessing the test application deployed called kuard and follow the rest of the setup instructions from the tofu apply output.\nRefer example output here to setup kubeconfig and access the cluster using kubectl.Now that your cluster is set up, you can proceed to installing Lifecycle application to your cluster.","amazon-web-services#Amazon Web Services":"*This profile needs to have access a user with AdministratorAccess access.","bootstrap-infrastructure-1#Bootstrap infrastructure":"Clone the infrastructure repository:\nFollow steps in the infrastructure repository to set up the necessary infrastructure components.\nExample secrets.auto.tfvars file:\nInitialize and apply the Terraform configuration:\nThis will create the necessary infrastructure components, including the Kubernetes cluster, DNS records, database, redis and other resources required for Lifecycle to function properly.After the Terraform apply completes, you should have a fully functional Kubernetes cluster with the necessary resources set up.Let's test the public DNS setup by accessing the test application deployed called kuard and follow the rest of the setup instructions from the tofu apply output.\nRefer example output here to setup kubeconfig and access the cluster using kubectl.Now that your cluster is set up, you can proceed to installing Lifecycle application to your cluster."}},"/docs/tips/using-mission-control":{"title":"Mission Control comment","data":{"":"Lifecycle uses Mission Control PR Comments to allow users to modify and customize their environments directly from the pull request comment. This enables easy service selection, branch customization, and environment variable overrides without modifying lifecycle.yaml.","selecting-and-deselecting-services#Selecting and Deselecting Services":"Each pull request environment includes default services and optional additional services. You can enable or disable services using the checkboxes.\nEnabled Services are marked with [x].\nDisabled Services are marked with [ ].\nExample:\nTo enable a service, change [ ] to [x]. To disable a service, change [x] to [ ]. As simple as that!\nIf you need to make multiple selections or deselections at once, use the\nEdit Comment option instead of clicking checkboxes individually. This\nprevents multiple back-to-back builds, as each selection triggers an event in\nLifecycle without deduplication.","choosing-a-branch#Choosing a Branch":"To deploy a specific branch for a service, modify the branch name after the service name.Example:\nThis will deploy frontend using the feature-branch instead of the default branch.","overriding-environment-variables#Overriding Environment Variables":"To set additional environment variables, use the Override Environment Variables section in the PR comment.Example:\nThis sets API_URL and CHIEF_INTERN in the environment without modifying the service configuration.","override-uuid#Override UUID":"To set a custom UUID (subdomain) for the environment, use the Override UUID section in the PR comment.\nReplace wagon-builder-060825 with your desired subdomain. This allows you to customize the environment URL without changing the underlying service configuration.Using the Mission Control PR Comment, you can easily customize your environment without modifying code, making it a flexible way to test and deploy changes dynamically."}},"/docs/tips/telemetry":{"title":"Telemetry","data":{"":"Lifecycle comes with built-in support for Datadog telemetry. To collect logs and metrics from your cluster and deployed applications, install the Datadog Agent and Cluster Agent in your cluster.The deployed applications are already configured with the necessary Datadog labels and environment variables for seamless integration:Pod labels:\nEnvironment variables:\nThis setup ensures that Datadog automatically detects the environment, service, and version for each application, enabling rich observability and correlation of logs and metrics in the Datadog platform."}},"/docs/troubleshooting/build-issues":{"title":"Troubleshooting Build Issues","data":{"":"TODO: This document will cover common build issues that you may encounter when\nworking with Lifecycle environments."}},"/docs/troubleshooting/github-app-webhooks":{"title":"Missing PR comment","data":{"":"Let's quickly validate that the app is able to send events to the Lifecycle app successfully.\nNavigate to your Github app\nClick App Settings link in the Github application page\nChoose Advanced from the left sidebar\nRecent Deliveries section should show a successful delivery of the installation event to the Lifecycle app.\nIf you see an error or no deliveries, make sure the app is installed in the\natleast one repository and that the webhook URL is set correctly by\nnavigating to the General section from the left sidebar and checking the\nWebhook URL field.\nIf the delivery is successful, you should see a status code of 200 OK","failing-deliveries#Failing deliveries":"If you see a delivery failure, it could be due to various reasons. Here are some common issues and how to resolve them:","github-app-secrets#Github App secrets":"Make sure that the Github App secrets are correctly set in the lifecycle-app namespace. You can verify this by running the following command:\nThe output should include all the GITHUB_* variables with the correct values.\nIf the secrets are present but the delivery is still failing, try restarting the following deployments.\nTry triggering a new event (create a pull request) by making a change in the repository or by manually redelivering a failed delivery."}},"/":{"title":"Lifecycle","data":{}},"/tags/agent":{"title":"agent","data":{}},"/tags/admin":{"title":"admin","data":{}},"/docs/troubleshooting/deploy-issues":{"title":"Deploy Issues","data":{"":"TODO: This document will cover common deploy issues that you may encounter\nwhen working with Lifecycle environments."}},"/tags/ai":{"title":"ai","data":{}},"/docs/what-is-lifecycle":{"title":"What is Lifecycle?","data":{"":"Lifecycle is an ephemeral (/…ôÀàfem(…ô)r…ôl/, lasting for a very short time) environment orchestrator that transforms your GitHub pull requests into fully functional development environments. It enables developers to test, validate, and collaborate on features without the hassle of managing infrastructure manually.\nWith Lifecycle, every pull request gets its own connected playground‚Äîensuring that changes can be previewed, integrated, and verified before merging into its main branch.","a-developers-story#A Developer‚Äôs Story":"Imagine working in an organization that develops multiple services. Managing and testing changes across these services can be challenging, especially when multiple developers are working on different features simultaneously.Meet Nick Holiday üë®‚Äçüíª, an engineer who needs to update a database schema and modify the corresponding API in a backend service. Additionally, his change requires frontend service updates to display the new data correctly.","traditional-workflow-challenges#Traditional Workflow Challenges":"Shared environments ‚Äì Nick deploys his backend service changes to a shared dev or staging environment, but another engineer is testing unrelated changes at the same time.\nConflicting updates ‚Äì The frontend engineers working on the UI might face issues if their code depends on a stable backend service that keeps changing.\nEnvironment management ‚Äì Setting up and maintaining an isolated environment for testing requires significant effort.","enter-lifecycle#Enter Lifecycle":"With Lifecycle, as soon as Nick opens a pull request, the system automatically:\nüèóÔ∏è Creates an isolated development environment ‚Äì This environment includes Nick‚Äôs updated backend service along with the necessary frontend services.\nüöÄ Deploys the application ‚Äì Everything is set up exactly as it would be in production, ensuring a reliable test scenario.\nüîó Generates a shareable URL ‚Äì Nick and his teammates can interact with the new features without setting up anything locally.\nüßπ Cleans up automatically ‚Äì Once the PR is merged or closed, Lifecycle removes the environment, keeping things tidy.","watch-a-quick-demo#Watch a Quick Demo":"","how-it-works#How It Works":"","why-use-lifecycle#Why Use Lifecycle?":"Faster Feedback Loops - Get instant previews of your changes without waiting for staging deployments.\nIsolation - Each PR runs in its own sandbox, preventing conflicts.\nSeamless Collaboration - Share URLs with stakeholders, designers, or QA engineers.\nAutomatic Cleanup - No more stale test environments; Lifecycle manages cleanup for you.\nWorks with Your Stack - Supports containerized applications and integrates with Kubernetes."}},"/tags/api":{"title":"api","data":{}},"/tags/app":{"title":"app","data":{}},"/tags/aurora":{"title":"aurora","data":{}},"/tags/automation":{"title":"automation","data":{}},"/tags/auto":{"title":"auto","data":{}},"/tags/aws":{"title":"aws","data":{}},"/tags/branchname":{"title":"branchname","data":{}},"/tags/build":{"title":"build","data":{}},"/tags/builduuid":{"title":"builduuid","data":{}},"/tags/ci-cd":{"title":"ci-cd","data":{}},"/tags/charts":{"title":"charts","data":{}},"/tags/cleanup":{"title":"cleanup","data":{}},"/tags/close":{"title":"close","data":{}},"/tags/codefresh":{"title":"codefresh","data":{}},"/tags/command":{"title":"command","data":{}},"/tags/cluster":{"title":"cluster","data":{}},"/tags/comment":{"title":"comment","data":{}},"/tags/core":{"title":"core","data":{}},"/tags/config":{"title":"config","data":{}},"/tags/configuration":{"title":"configuration","data":{}},"/tags/configure":{"title":"configure","data":{}},"/tags/database":{"title":"database","data":{}},"/tags/datadog":{"title":"datadog","data":{}},"/tags/debugging":{"title":"debugging","data":{}},"/tags/defaultservices":{"title":"defaultservices","data":{}},"/tags/delete":{"title":"delete","data":{}},"/tags/deploy":{"title":"deploy","data":{}},"/tags/deployment":{"title":"deployment","data":{}},"/tags/dev-0":{"title":"dev-0","data":{}},"/tags/disabled":{"title":"disabled","data":{}},"/tags/docker":{"title":"docker","data":{}},"/tags/environment variables":{"title":"environment variables","data":{}},"/tags/environment":{"title":"environment","data":{}},"/tags/edit":{"title":"edit","data":{}},"/tags/error":{"title":"error","data":{}},"/tags/ephemeral-env":{"title":"ephemeral-env","data":{}},"/tags/explore":{"title":"explore","data":{}},"/tags/feature-flags":{"title":"feature-flags","data":{}},"/tags/first environment":{"title":"first environment","data":{}},"/tags/gcp":{"title":"gcp","data":{}},"/tags/getting-started":{"title":"getting-started","data":{}},"/tags/gke":{"title":"gke","data":{}},"/tags/github":{"title":"github","data":{}},"/tags/global-config":{"title":"global-config","data":{}},"/tags/glossary":{"title":"glossary","data":{}},"/tags/helm":{"title":"helm","data":{}},"/tags/inactivity":{"title":"inactivity","data":{}},"/tags/install":{"title":"install","data":{}},"/tags/integration":{"title":"integration","data":{}},"/tags/issue":{"title":"issue","data":{}},"/tags/internalhostname":{"title":"internalhostname","data":{}},"/tags/investigation":{"title":"investigation","data":{}},"/tags/intro":{"title":"intro","data":{}},"/tags/issues":{"title":"issues","data":{}},"/tags/keep":{"title":"keep","data":{}},"/tags/kubernetes":{"title":"kubernetes","data":{}},"/tags/labels":{"title":"labels","data":{}},"/tags/lifecycle-keep":{"title":"lifecycle-keep","data":{}},"/tags/lifecycle":{"title":"lifecycle","data":{}},"/tags/lifecycle.yaml":{"title":"lifecycle.yaml","data":{}},"/tags/lifecycle-disabled":{"title":"lifecycle-disabled","data":{}},"/tags/llm":{"title":"llm","data":{}},"/tags/logs":{"title":"logs","data":{}},"/tags/metrics":{"title":"metrics","data":{}},"/tags/missing":{"title":"missing","data":{}},"/tags/mcp":{"title":"mcp","data":{}},"/tags/native":{"title":"native","data":{}},"/tags/mission control":{"title":"mission control","data":{}},"/tags/onboard":{"title":"onboard","data":{}},"/tags/observability":{"title":"observability","data":{}},"/tags/optionalservices":{"title":"optionalservices","data":{}},"/tags/pipeline":{"title":"pipeline","data":{}},"/tags/pr":{"title":"pr","data":{}},"/tags/postgres":{"title":"postgres","data":{}},"/tags/publicurl":{"title":"publicurl","data":{}},"/tags/prerequisites":{"title":"prerequisites","data":{}},"/tags/pull request":{"title":"pull request","data":{}},"/tags/redis":{"title":"redis","data":{}},"/tags/requirements":{"title":"requirements","data":{}},"/tags/restore":{"title":"restore","data":{}},"/tags/review":{"title":"review","data":{}},"/tags/secrets":{"title":"secrets","data":{}},"/tags/schema":{"title":"schema","data":{}},"/tags/security":{"title":"security","data":{}},"/tags/service-dependencies":{"title":"service-dependencies","data":{}},"/tags/services":{"title":"services","data":{}},"/tags/service":{"title":"service","data":{}},"/tags/setup":{"title":"setup","data":{}},"/tags/sha":{"title":"sha","data":{}},"/tags/static":{"title":"static","data":{}},"/tags/start":{"title":"start","data":{}},"/tags/status-comments":{"title":"status-comments","data":{}},"/tags/tear down":{"title":"tear down","data":{}},"/tags/telemetry":{"title":"telemetry","data":{}},"/tags/template":{"title":"template","data":{}},"/tags/todo":{"title":"todo","data":{}},"/tags/term":{"title":"term","data":{}},"/tags/tools":{"title":"tools","data":{}},"/tags/terminology":{"title":"terminology","data":{}},"/tags/ttl":{"title":"ttl","data":{}},"/tags/tutorial":{"title":"tutorial","data":{}},"/tags/uuid":{"title":"uuid","data":{}},"/tags/variables":{"title":"variables","data":{}},"/tags/webhooks":{"title":"webhooks","data":{}},"/tags/webhook":{"title":"webhook","data":{}},"/docs/features/ai-agent-configuration":{"title":"AI Agent Configuration","data":{"":"The AI Agent configuration system gives administrators fine-grained control over agent behavior at two levels: global defaults that apply to all repositories, and per-repository overrides for specific projects. All configuration is managed through REST API endpoints.\nFor an overview of the AI Agent itself, see AI\nAgent. To extend the agent with external tools, see\nMCP Integration.","configuration-hierarchy#Configuration hierarchy":"Global defaults live in the Lifecycle global_config table under the aiAgent key. Per-repository overrides are stored in the ai_agent_repo_config table and merged on top of global defaults at runtime.","fields#Fields":"Field\tType\tDefault\tDescription\tenabled\tboolean\tfalse\tWhether the AI Agent is available\tmaxMessagesPerSession\tnumber\t50\tMaximum messages per chat session\tsessionTTL\tnumber\t3600\tSession time-to-live in seconds\tproviders\tProviderConfig[]\t[]\tLLM provider configurations (global only)\tadditiveRules\tstring[]\t[]\tExtra rules appended to the system prompt\tsystemPromptOverride\tstring\tundefined\tFull replacement for the system prompt\texcludedTools\tstring[]\t[]\tTools the agent cannot use\texcludedFilePatterns\tstring[]\t[]\tGlob patterns for files the agent cannot access\tallowedWritePatterns\tstring[]\t[\"lifecycle.yaml\", \"lifecycle.yml\"]\tGlob patterns for additional file paths the agent is allowed to write to\tmaxIterations\tnumber\t20\tMaximum orchestration loop iterations (global only)\tmaxToolCalls\tnumber\t50\tMaximum total tool calls per query (global only)\tmaxRepeatedCalls\tnumber\t1\tMaximum repeated calls with same arguments before loop detection (global only)\tcompressionThreshold\tnumber\t80000\tToken count before conversation history is compressed (global only)\tobservationMaskingRecencyWindow\tnumber\t3\tNumber of recent tool results preserved when masking (global only)\tobservationMaskingTokenThreshold\tnumber\t25000\tToken count before observation masking activates (global only)\ttoolExecutionTimeout\tnumber\t30000\tTool execution timeout in milliseconds (global only)\ttoolOutputMaxChars\tnumber\t30000\tMaximum characters in tool output before truncation (global only)\tretryBudget\tnumber\t10\tMaximum retry attempts per query on provider errors (global only)","how-merging-works#How merging works":"When a repository has an override, the effective configuration is computed by merging the override on top of global defaults:\nScalar fields (enabled, maxMessagesPerSession, sessionTTL, systemPromptOverride) ‚Äî the repository value replaces the global value.\nArray fields (additiveRules, excludedTools, excludedFilePatterns, allowedWritePatterns) ‚Äî repository values are appended to global values. Duplicates are removed automatically.\nHere's a concrete example. Say your global config looks like this:\nAnd myorg/frontend has this override:\nThe effective config for myorg/frontend:","provider-and-model-configuration#Provider and model configuration":"The providers field defines which LLM providers and models are available. This is a global-only field ‚Äî repository overrides cannot change provider configuration.Each provider entry has the following structure:\nField\tType\tRequired\tDescription\tname\tstring\tYes\tProvider identifier (anthropic, openai, gemini)\tenabled\tboolean\tYes\tWhether this provider is available\tapiKeyEnvVar\tstring\tYes\tEnvironment variable containing the API key\tmodels\tModelConfig[]\tYes\tList of models for this provider\t\nEach model entry:\nField\tType\tRequired\tDescription\tid\tstring\tYes\tModel identifier sent to the provider API\tdisplayName\tstring\tYes\tHuman-readable name shown in the UI dropdown\tenabled\tboolean\tYes\tWhether this model is selectable\tdefault\tboolean\tYes\tWhether this is the default model (one per provider)\tmaxTokens\tinteger\tYes\tMaximum output tokens for the model\tinputCostPerMillion\tnumber\tNo\tCost per 1M input tokens (USD). Enables cost display in the UI\toutputCostPerMillion\tnumber\tNo\tCost per 1M output tokens (USD). Enables cost display in the UI\t\nWhen inputCostPerMillion and outputCostPerMillion are both set for a\nmodel, the UI displays a computed cost alongside token counts in the X-Ray\ndebug panel. If either field is omitted, cost is not shown.\nExample with pricing configured:","caching#Caching":"Configurations are cached at three layers: in-memory (30 seconds), Redis (5\nminutes), and the database (source of truth). Updating via the API immediately\ninvalidates both the Redis and in-memory caches for that repository.","prompt-customization#Prompt customization":"","additive-rules#Additive rules":"The additiveRules field accepts an array of strings appended to the system prompt. Use this to add organization-specific guidelines, restrict behaviors, or provide extra context.Because additive rules use additive merge, global rules and repository rules are combined. Set organization-wide rules globally and layer project-specific rules per repository.","system-prompt-override#System prompt override":"The systemPromptOverride field replaces the entire default system prompt. This is a full replacement, not additive.\nSetting systemPromptOverride replaces the entire built-in system prompt,\nincluding safety instructions. Use this only when you need complete control\nover the agent's behavior. Maximum length is 50,000 characters.\nWhen set at the repository level, the override applies only to that repository. The scalar merge rule means the repository value fully replaces any global override.","access-control#Access control":"","excluded-tools#Excluded tools":"The excludedTools field prevents the agent from using specific tools. This is useful for restricting write operations in sensitive repositories.Available tool names:\nget_k8s_resources\nget_lifecycle_logs\npatch_k8s_resource\nquery_database\nget_github_file\nlist_github_directory\nupdate_github_file\nget_issue_comment\nquery_database is a core tool and cannot be excluded. Attempting to\nexclude it returns a 400 error.\nBecause excluded tools use additive merge, tools excluded at the global level cannot be \"un-excluded\" at the repository level.","excluded-file-patterns#Excluded file patterns":"The excludedFilePatterns field accepts glob patterns that restrict which files the agent can access through the GitHub tools.Validation rules:\nRule\tLimit\tMaximum number of patterns\t50\tMaximum pattern length\t200 characters\tForbidden patterns\t*, **, **/* (overly broad)\tPath traversal\t../ is not allowed\tAbsolute paths\tPatterns starting with / not allowed\t\nLike other array fields, file patterns use additive merge. Global patterns and repository patterns are combined and deduplicated.","allowed-write-patterns#Allowed write patterns":"The allowedWritePatterns field controls which file paths the agent is permitted to modify through the update_file tool. By default, the agent can only write to lifecycle.yaml and lifecycle.yml (plus any files explicitly referenced in the lifecycle configuration such as Dockerfiles and Helm value files).To allow the agent to modify additional files, add glob patterns to this field. For example, to allow modifications to Helm charts, Dockerfiles, and Ansible playbooks:\nSetting allowedWritePatterns at the global or repository level defines the\nfull set of writable paths (in addition to files referenced in the lifecycle\nconfig). Keep the list minimal to limit the blast radius of agent changes.\nLike other array fields, allowed write patterns use additive merge. Global patterns and repository patterns are combined and deduplicated.","orchestration-limits#Orchestration limits":"These fields control the agent's internal behavior. They are all global-only settings ‚Äî repository overrides cannot change them. If omitted, the defaults below are used.","loop-protection#Loop protection":"Controls for the agent's tool-calling loop that prevent runaway behavior.\nField\tDefault\tDescription\tmaxIterations\t20\tMaximum number of LLM round-trips in a single query. Prevents runaway conversations.\tmaxToolCalls\t50\tMaximum total tool invocations across all iterations. Caps resource usage for broad investigations.\tmaxRepeatedCalls\t1\tHow many times the same tool can be called with identical arguments before loop detection kicks in.\t\nWhen a limit is hit, the agent stops and returns an error message to the user explaining what happened.","context-management#Context management":"Controls how the agent manages conversation context to stay within LLM token limits.\nField\tDefault\tDescription\tcompressionThreshold\t80000\tToken count at which the conversation history is summarized into a compact form. Higher values preserve more context but use more tokens.\tobservationMaskingRecencyWindow\t3\tNumber of recent tool results kept in full when masking older observations. Older results are replaced with placeholders.\tobservationMaskingTokenThreshold\t25000\tToken count at which observation masking activates. Below this threshold, all tool results are kept in full.","tool-execution#Tool execution":"Controls for tool execution behavior and output handling.\nField\tDefault\tDescription\ttoolExecutionTimeout\t30000\tMaximum time in milliseconds a single tool call can run before being terminated. Increase for slow K8s clusters or MCP servers.\ttoolOutputMaxChars\t30000\tMaximum characters in a tool's output before truncation. Larger values give the agent more data but cost more tokens.","resilience#Resilience":"Controls for error recovery when communicating with LLM providers.\nField\tDefault\tDescription\tretryBudget\t10\tMaximum retry attempts per query when the LLM provider returns transient errors. Higher values improve reliability at the cost of latency.\t\nIncreasing these limits allows the agent to perform deeper investigations but\nconsumes more LLM tokens. Lower values are safer for cost control.","managing-configuration-via-api#Managing configuration via API":"All endpoints are under /api/v2/ai/agent-config. Request and response bodies use JSON.","global-configuration#Global configuration":"Get the current global config:\nUpdate the global config:\nReturns the updated configuration on success. Returns 400 for validation failures (invalid schema, prompt too long, core tool exclusion, bad file patterns).","repository-overrides#Repository overrides":"List all repository overrides:\nGet a single repository override:\nReturns only the override fields, not the merged result. Returns 404 if no override exists.Get the effective (merged) configuration:\nUse the /effective endpoint to verify exactly what configuration a\nrepository uses at runtime, after merge.\nCreate or update a repository override:\nUses upsert semantics. Only include the fields you want to override.\nReturns the saved override on success. Returns 400 for validation failures.Delete a repository override:\nSoft-deletes the override. The repository reverts to global defaults.","troubleshooting#Troubleshooting":"","config-changes-not-taking-effect#Config changes not taking effect":"Configuration is cached at multiple layers. The API automatically invalidates caches on update, but if you changed the database directly, you may need to wait:\nIn-memory cache ‚Äî expires after 30 seconds\nRedis cache ‚Äî expires after 5 minutes\nUse the /effective endpoint to confirm what the agent sees.","cannot-exclude-a-core-tool#Cannot exclude a core tool":"query_database is required for agent operation. If you include it in excludedTools, the API returns:\nRemove it from your exclusion list.","excluded-tool-still-being-used#Excluded tool still being used":"Array fields use additive merge. If you removed a tool from a repository override but it's still excluded, check the global config ‚Äî the tool may be excluded there. Global exclusions cannot be overridden at the repo level.","system-prompt-override-not-working-as-expected#System prompt override not working as expected":"systemPromptOverride completely replaces the built-in prompt, including safety instructions and tool usage guidelines. If the agent behaves unexpectedly, try additiveRules instead ‚Äî it appends to the default prompt rather than replacing it."}},"/docs/features/secrets":{"title":"Cloud Secrets","data":{"overview#Overview":"Lifecycle integrates with cloud secret providers to securely inject secrets into your ephemeral environments. You can reference secrets directly in your lifecycle.yaml environment variables using a template syntax, and Lifecycle handles the rest.Supported providers:\nAWS Secrets Manager\nGCP Secret Manager","configuration#Configuration":"Secret providers are configured in the Lifecycle global_config database table under the secretProviders key.","default-configuration#Default Configuration":"Both AWS and GCP providers are enabled by default. The ClusterSecretStore created by your platform team determines which provider actually works in your cluster.\nOn AWS (EKS): Use {{aws:path:key}} syntax. The aws-secretsmanager ClusterSecretStore handles the request.\nOn GCP (GKE): Use {{gcp:path:key}} syntax. The gcp-secretmanager ClusterSecretStore handles the request.\nIf you reference a provider that doesn't have a ClusterSecretStore in your cluster, the secret will fail to sync and a warning will be logged.","configuration-fields#Configuration Fields":"Field\tRequired\tDescription\tenabled\tYes\tEnable this provider\tclusterSecretStore\tYes\tName of the ClusterSecretStore CR configured by your platform team\trefreshInterval\tYes\tHow often ESO syncs secrets (e.g., 1h, 30m)\tallowedPrefixes\tNo\tRestrict which secret paths can be referenced (empty = allow all)\t\nIf you're unsure which clusterSecretStore to use, check with your platform\nteam. They configure the External Secrets Operator and ClusterSecretStore as\npart of cluster setup.","syntax#Syntax":"Reference secrets using the following pattern:\nComponent\tDescription\tExample\tprovider\tCloud provider (aws or gcp)\taws\tsecret-path\tFull path to the secret\tmyapp/database-credentials\tkey\tJSON key within the secret (optional)\tpassword\t\nIf your secret is a plaintext value (not JSON), omit the key portion: \n  {\"{{aws:myapp/api-key}}\"}.","basic-usage#Basic Usage":"","json-secrets#JSON Secrets":"For secrets stored as JSON objects, specify the key to extract:\nIf your AWS secret myapp/rds-credentials contains:\nYour pod will receive:\nDB_USERNAME=admin\nDB_PASSWORD=supersecret\nDB_HOST=db.example.com","plaintext-secrets#Plaintext Secrets":"For secrets stored as plain strings, omit the key:","nested-json#Nested JSON":"For nested JSON structures, use dot notation:\nFor a secret containing:","where-secrets-work#Where Secrets Work":"The secret syntax works in all env blocks:","github-services#GitHub Services":"","docker-services#Docker Services":"","webhooks#Webhooks":"Helm and Codefresh deployment types do not currently support cloud secrets.","mixing-secrets-with-template-variables#Mixing Secrets with Template Variables":"You can combine cloud secrets with regular template variables in the same env block:","multiple-providers#Multiple Providers":"Both providers are enabled by default. If your platform team has configured ClusterSecretStores for both AWS and GCP, you can reference secrets from either in the same deployment:\nEach provider requires its own ClusterSecretStore. Contact your platform team\nif you need multi-cloud secret access.","error-handling#Error Handling":"Lifecycle uses a \"warn and continue\" approach for secret errors:\nScenario\tBehavior\tSecret not found\tWarning logged, env var not set, deployment continues\tKey not found in JSON\tWarning logged, env var not set\tProvider not enabled\tWarning logged, env var not set\tInvalid syntax\tValidation error at deploy time\t\nIf a required secret is missing, your application may fail to start or behave\nunexpectedly. Always verify your secrets exist before deploying.","best-practices#Best Practices":"","use-descriptive-secret-paths#Use Descriptive Secret Paths":"Organize secrets with meaningful paths:","group-related-secrets#Group Related Secrets":"Store related credentials in a single JSON secret:\nThen reference individual keys:","avoid-secrets-in-build-logs#Avoid Secrets in Build Logs":"Be cautious when using secrets in build-time environment variables, as they may appear in build logs. Prefer injecting secrets at runtime when possible.","troubleshooting#Troubleshooting":"","secret-not-injected#Secret Not Injected":"Check the secret exists in AWS Secrets Manager / GCP Secret Manager\nVerify the path matches exactly (case-sensitive)\nCheck the key name if using JSON secrets\nReview Lifecycle logs for warnings about missing secrets\nVerify provider is enabled in global configuration","syntax-errors#Syntax Errors":"Common syntax mistakes:","permission-denied#Permission Denied":"If you see permission errors:\nVerify the External Secrets Operator has access to the secret path\nCheck IAM policies allow access to the specific secret\nContact your platform team if the secret is in a restricted path","wrong-provider-for-cluster#Wrong Provider for Cluster":"If you use {{gcp:...}} on an AWS cluster (or vice versa), the ExternalSecret will fail to sync because the ClusterSecretStore doesn't exist. Check Lifecycle logs for warnings like \"ClusterSecretStore not found\".","prerequisites#Prerequisites":"For cloud secrets to work, your platform team must set up:\nComponent\tDescription\tExternal Secrets Operator\tKubernetes operator that syncs secrets from cloud providers\tClusterSecretStore\tCR that configures ESO to connect to AWS Secrets Manager or GCP Secret Manager\tIAM/Workload Identity\tPermissions for ESO to read secrets (IRSA for AWS, Workload Identity for GCP)\t\nThe clusterSecretStore value in your Lifecycle config must match the ClusterSecretStore name configured by your platform team.","build-time-secrets#Build-Time Secrets":"The cloud secrets described above are runtime secrets ‚Äî they get mounted into your running pods. Lifecycle also supports build-time secrets, which are injected during the Docker image build as build arguments.You use the same {{provider:path:key}} syntax. The difference is where the secret is consumed: build-time secrets are available during docker build, while runtime secrets are mounted into the final running container.Common use cases:\nPrivate NPM registry tokens (e.g., NPM_TOKEN for .npmrc)\nPrivate package repository credentials\nLicense keys required during compilation\nAuthentication tokens for private base images","configuration-1#Configuration":"Define build-time secrets in the env block of your service's docker configuration, just like runtime secrets:\nYour Dockerfile then references them as build arguments:","how-it-works#How It Works":"When you reference a {{provider:path:key}} value in a build service's env block, Lifecycle resolves the secret from your cloud provider before the build starts. The resolved values are passed to Docker as build arguments (--build-arg), making them available as ARG variables in your Dockerfile.Lifecycle waits for the secret to sync before starting the build, so you don't need to worry about race conditions with the External Secrets Operator.\nBoth Buildkit and Kaniko build engines support build-time secrets. The secret\nresolution process is the same regardless of which engine you use.","security-considerations#Security Considerations":"Build-time secrets are not masked in build logs. Any RUN command that\nprints environment variables (e.g., echo $NPM_TOKEN, env, printenv) will\nexpose secret values in the build output. Build logs are fetched on-demand\nfrom Kubernetes and are visible to anyone with access to the Lifecycle UI.\nTo minimize exposure:\nNever print environment variables during build steps\nDelete credential files after use (e.g., RUN rm -f .npmrc after npm install)\nUse multi-stage builds so secrets are only present in the builder stage, not the final image\nPrefer runtime secrets when possible ‚Äî only use build-time secrets when the value is genuinely needed during image construction","troubleshooting-1#Troubleshooting":"","secret-not-available-during-build#Secret Not Available During Build":"Verify the syntax ‚Äî make sure your env var uses the {{provider:path:key}} pattern\nCheck that the secret exists in AWS Secrets Manager or GCP Secret Manager\nConfirm the ExternalSecret synced ‚Äî Lifecycle waits for sync before building, but if the secret path is wrong or permissions are missing, the sync will fail\nCheck build logs for warnings about missing or unresolved secrets","secret-value-is-empty#Secret Value Is Empty":"If your secret is stored as a plaintext string (not JSON), you can omit the key. But if the secret is JSON, you must specify which key to extract.","dockerfile-not-receiving-the-value#Dockerfile Not Receiving the Value":"Make sure your Dockerfile declares the build argument with ARG:","related#Related":"Template Variables - Learn about other template syntax\nWebhooks - Using secrets in webhook environment variables\nNative Builds - Build pipeline that consumes build-time secrets"}},"/tags/lifecycle-deploy":{"title":"lifecycle-deploy","data":{}}}