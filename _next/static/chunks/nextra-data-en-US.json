{"/docs/features/auto-deployment":{"title":"Auto Deploy & Labels","data":{"auto-deploy-configuration#Auto-Deploy Configuration":"To enable automatic deployment when a PR is created, set the autoDeploy attribute in your repository's lifecycle.yaml file:\nLifecycle will automatically create the environment as soon as a PR is opened.\nA lifecycle-deploy! label will be added to the PR to indicate that the environment has been deployed.","managing-deployments-with-labels#Managing Deployments with Labels":"If auto-deploy is not enabled, you can manually control the environment using PR labels.","deploy-an-environment#Deploy an Environment":"To create an ephemeral environment for a PR, add the lifecycle-deploy! label.","tear-down-an-environment#Tear Down an Environment":"To delete an active environment, use either of these labels:\nRemove lifecycle-deploy!\nAdd lifecycle-disabled!","automatic-cleanup-on-pr-closure#Automatic Cleanup on PR Closure":"When a PR is closed, Lifecycle will:\nTear down the active environment.\nRemove the lifecycle-deploy! label from the PR.\nThis ensures that unused environments do not persist after the PR lifecycle is complete.","summary#Summary":"Feature\tBehavior\tautoDeploy: true in config\tPR environments are automatically deployed.\tlifecycle-deploy!\tManually deploy an environment.\tRemove lifecycle-deploy!\tTear down the environment.\tAdd lifecycle-disabled!\tTear down the environment manually.\tPR closed\tEnvironment is deleted automatically.\t\nUsing these configurations and labels, teams can efficiently manage ephemeral environments in their development workflow."}},"/docs/features/native-helm-deployment":{"title":"Native Helm Deployment","data":{"":"This feature is still in alpha and might change with breaking changes.\nNative Helm is an alternative deployment method that runs Helm deployments directly within Kubernetes jobs, eliminating the need for external CI/CD systems. This provides a more self-contained and portable deployment solution.\nNative Helm deployment is an opt-in feature that can be enabled globally or\nper-service.","overview#Overview":"When enabled, Native Helm:\nCreates Kubernetes jobs to execute Helm deployments\nRuns in ephemeral namespaces with proper RBAC\nProvides real-time deployment logs via WebSocket\nHandles concurrent deployments automatically\nSupports all standard Helm chart types","quickstart#Quickstart":"Want to try native Helm deployment? Here's the fastest way to get started:\nThis configuration:\nEnables native Helm for the my-api service\nUses a local Helm chart from your repository\nApplies values from ./helm/values.yaml\nRuns deployment as a Kubernetes job\nTo enable native Helm for all services at once, see Global\nConfiguration.","configuration#Configuration":"","enabling-native-helm#Enabling Native Helm":"There are two ways to enable native Helm deployment:","per-service-configuration#Per Service Configuration":"Enable native Helm for individual services:","global-configuration#Global Configuration":"Enable native Helm for all services:","configuration-precedence#Configuration Precedence":"Lifecycle uses a hierarchical configuration system with three levels of precedence:\nhelmDefaults - Base defaults for all deployments (database: global_config table)\nChart-specific config - Per-chart defaults (database: global_config table)\nService YAML config - Service-specific overrides (highest priority)\nService-level configuration always takes precedence over global defaults.","global-configuration-database#Global Configuration (Database)":"Global configurations are stored in the global_config table in the database. Each configuration is stored as a row with:\nkey: The configuration name (e.g., 'helmDefaults', 'postgresql', 'redis')\nconfig: JSON object containing the configuration","helmdefaults-configuration#helmDefaults Configuration":"Stored in database with key helmDefaults:\nField Descriptions:\nenabled: When true, enables native Helm deployment for all services unless they explicitly set deploymentMethod: \"ci\"\ndefaultArgs: Arguments automatically appended to every Helm command (appears before service-specific args)\ndefaultHelmVersion: The Helm version to use when not specified at the service or chart level","chart-specific-configuration#Chart-specific Configuration":"Example: PostgreSQL configuration stored with key postgresql:\nThese global configurations are managed by administrators and stored in the\ndatabase. They provide consistent defaults across all environments and can be\noverridden at the service level.","usage-examples#Usage Examples":"","quick-experiment-deploy-jenkins#Quick Experiment: Deploy Jenkins!":"Want to see native Helm in action? Let's deploy everyone's favorite CI/CD tool - Jenkins! This example shows how easy it is to deploy popular applications using native Helm.\nüéâ That's it! With just a few lines of configuration, you'll have Jenkins\nrunning in your Kubernetes cluster.\nTo access your Jenkins instance:\nCheck the deployment status in your PR comment\nClick the Deploy Logs link to monitor the deployment\nOnce deployed, Jenkins will be available at the internal hostname\nFor more Jenkins configuration options and values, check out the Bitnami\nJenkins chart\ndocumentation.\nThis same pattern works for any Bitnami chart (PostgreSQL, Redis, MongoDB) or\nany other public Helm chart!","basic-service-deployment#Basic Service Deployment":"","postgresql-with-overrides#PostgreSQL with Overrides":"","custom-environment-variables#Custom Environment Variables":"Lifecycle supports flexible environment variable formatting through the envMapping configuration. This feature allows you to control how environment variables from your service configuration are passed to your Helm chart.\nWhy envMapping? Different Helm charts expect environment variables in\ndifferent formats. Some expect an array of objects with name and value\nfields (Kubernetes standard), while others expect a simple key-value map. The\nenvMapping feature lets you adapt to your chart's requirements.","default-envmapping-configuration#Default envMapping Configuration":"You can define default envMapping configurations in the global_config database table. These defaults apply to all services using that chart unless overridden at the service level.Example: Setting defaults for your organization's chart\nWith this configuration, any service using the myorg-web-app chart will automatically use array format for environment variables:\nSetting envMapping in global_config is particularly useful when: - You have\na standard organizational chart used by many services - You want consistent\nenvironment variable handling across services - You're migrating multiple\nservices and want to reduce configuration duplication","array-format#Array Format":"Best for charts that expect Kubernetes-style env arrays.\nThis produces the following Helm values:\nYour chart's values.yaml would use it like:","map-format#Map Format":"Best for charts that expect a simple key-value object.\nThis produces the following Helm values:\nNote: Underscores in environment variable names are converted to double\nunderscores (__) in map format to avoid Helm parsing issues.\nYour chart's values.yaml would use it like:","complete-example-with-multiple-services#Complete Example with Multiple Services":"","templated-variables#Templated Variables":"Lifecycle supports template variables in Helm values that are resolved at deployment time. These variables allow you to reference dynamic values like build UUIDs, docker tags, and internal hostnames.","available-variables#Available Variables":"Template variables use the format {{{variableName}}} and are replaced with actual values during deployment:\nVariable\tDescription\tExample Value\t{{{serviceName_dockerTag}}}\tDocker tag for a service\tmain-abc123\t{{{serviceName_dockerImage}}}\tFull docker image path\tregistry.com/org/repo:main-abc123\t{{{serviceName_internalHostname}}}\tInternal service hostname\tapi-service.env-uuid.svc.cluster.local\t{{{build.uuid}}}\tBuild UUID\tenv-12345\t{{{build.namespace}}}\tKubernetes namespace\tenv-12345","usage-in-values#Usage in Values":"Docker Image Mapping: When using custom charts, you'll need to map {{{serviceName_dockerImage}}} or {{{serviceName_dockerTag}}} to your chart's expected value path. Common patterns include:\nimage.repository and image.tag (most common)\ndeployment.image (single image string)\napp.image or application.image\nCustom paths specific to your chart\nCheck your chart's values.yaml to determine the correct path.","image-mapping-examples#Image Mapping Examples":"Important: Always use triple braces {{{variable}}} instead of double braces {{variable}} for Lifecycle template variables. This prevents Helm from trying to process them as Helm template functions and ensures they are passed through correctly for Lifecycle to resolve.","template-resolution-order#Template Resolution Order":"Lifecycle resolves {{{variables}}} before passing values to Helm\nThe resolved values are then passed to Helm using --set flags\nHelm processes its own template functions (if any) after receiving the resolved values","example-with-service-dependencies#Example with Service Dependencies":"","deployment-process#Deployment Process":"Job Creation: A Kubernetes job is created in the ephemeral namespace 2.\nRBAC Setup: Service account with namespace-scoped permissions is created\nGit Clone: Init container clones the repository 4. Helm Deploy:\nMain container executes the Helm deployment 5. Monitoring: Logs are\nstreamed in real-time via WebSocket","concurrent-deployment-handling#Concurrent Deployment Handling":"Native Helm automatically handles concurrent deployments by:\nDetecting existing deployment jobs\nForce-deleting the old job\nStarting the new deployment\nThis ensures the newest deployment always takes precedence.","monitoring-deployments#Monitoring Deployments":"","deploy-logs-access#Deploy Logs Access":"For services using native Helm deployment, you can access deployment logs through the Lifecycle PR comment:\nAdd the lifecycle-status-comments! label to your PR\nIn the status comment that appears, you'll see a Deploy Logs link for each service using native Helm\nClick the link to view real-time deployment logs","log-contents#Log Contents":"The deployment logs show:\nGit repository cloning progress (clone-repo container)\nHelm deployment execution (helm-deploy container)\nReal-time streaming of all deployment output\nSuccess or failure status","chart-types#Chart Types":"Lifecycle automatically detects and handles three chart types:\nType\tDetection\tFeatures\tORG_CHART\tMatches orgChartName AND has helm.docker\tDocker image injection, env var transformation\tLOCAL\tName is \"local\" or starts with \"./\" or \"../\"\tFlexible envMapping support\tPUBLIC\tEverything else\tStandard labels and tolerations\t\nThe orgChartName is configured in the database's global_config table with\nkey orgChart. This allows organizations to define their standard internal\nHelm chart.","troubleshooting#Troubleshooting":"","deployment-fails-with-another-operation-in-progress#Deployment Fails with \"Another Operation in Progress\"":"Symptom: Helm reports an existing operation is blocking deploymentSolution: Native Helm automatically handles this by killing existing jobs. If the issue persists:","environment-variables-not-working#Environment Variables Not Working":"Symptom: Environment variables not passed to the deploymentCommon Issues:\nenvMapping placed under chart instead of directly under helm\nIncorrect format specification (array vs map)\nMissing path configuration\nCorrect Configuration:","migration-example#Migration Example":"Here's a complete example showing how to migrate from GitHub-type services to Helm-type services:","before-github-type-services#Before: GitHub-type Services":"","after-helm-type-services-with-native-deployment#After: Helm-type Services with Native Deployment":"","key-migration-points#Key Migration Points":"Service Type Change: Changed from github: to helm: configuration\nRepository Location: repository and branchName move from under github: to directly under helm:\nDeployment Method: Added deploymentMethod: \"native\" to enable native Helm\nChart Configuration: Added chart: section with local or public charts\nEnvironment Mapping: Added envMapping: to control how environment variables are passed\nHelm Arguments: Added args: for Helm command customization\nDocker Configuration: Kept existing docker: config for build process\nNote that when converting from GitHub-type to Helm-type services, the\nrepository and branchName fields move from being nested under github: to\nbeing directly under helm:.\nMany configuration options (like Helm version, args, and chart details) can be\ndefined in the global_config database table, making the service YAML\ncleaner. Only override when needed."}},"/docs/features/service-dependencies":{"title":"Service Dependencies","data":{"":"This document will cover environment.{defaultServices,optionalServices} and service.requires, their differences, impact scope, and usage.","environmentdefaultservicesoptionalservices#environment.{defaultServices,optionalServices}":"","impact-scope#Impact scope":"Scope\tImpact\tService repo*\t‚úÖ\tOutside repo*\t‚ùå\tdev-0*\t‚ùå\t\nThis represents the default environment that will be created by lifecycle when a pull request is opened in the service repo* and does not have any impact on outside repos, dev-0, or any other static environments that use this service.","servicesrequires#services.requires":"","impact-scope-1#Impact scope":"Scope\tImpact\tService repo*\t‚úÖ\tOutside repo*\t‚úÖ\tdev-0*\t‚úÖ\t\nservices.requires has an impact across the board; hence, it is important to understand how it works and when we should use them.Please read the info blocks below carefully.You can think of services.requires as a hard dependency definition. For example, if you have an API service and a database, the API service will have a hard dependency on the database.\nIn this scenario, the database should not be defined as the default service. Instead, we should make the dependency explicitly clear by adding the database to the API‚Äôs requires block.\nBy doing this, we ensure that any outside repo that wants to use our API service will get the database along with it but only needs to specify the API service in their defaultServices or optionalServices.\nOnly services defined in lifecycle.yaml should be used in the requires\narray. If a service is defined in an outside repo, use\nenvironment.defaultServices instead.\nDo not use services in the services.requires if the service itself is not\ndefined in the same lifecycle.yaml.\nServices defined in the requires block will only be resolved 1 level down.\nThis is a very important nuance, which we get tripped by regularly.","examples#Examples":"To better illustrate the above statement, consider this example.Repository A r-A has 3 services s-A, s-B, and s-C.\ns-A requires s-B.\ns-B requires s-C.\nAs you can see, s-A has an indirect dependency on s-C through s-B.","scenario-1-pull-request-in-service-repo-#Scenario 1: Pull Request in Service repo* ‚úÖ":"When we open a pull request in r-A repo, lifecycle will deploy 3 services: s-A, s-B, and s-C.","breakdown#Breakdown":"Lifecycle deploys s-A and s-B because they are defined in defaultServices.\nServices defined in the requires block will only be resolved one level down.\nOnly services defined in lifecycle.yaml should be used in the requires array. If a service is defined in an outside repo, use environment.defaultServices instead.","scenario-2-#Scenario 2: ‚ùå":"Repository B r-B has service s-X and also defines an outside repo r-A service s-A as environment.defaultServices.","breakdown-1#Breakdown":"Lifecycle deploys s-X and s-A because they are defined in defaultServices.\nLifecycle deploys s-B because it is a 1st level dependency of a service (s-A) listed in defaultServices.\nLifecycle does not deploy s-C since it is not a 1st level dependency of any service listed in defaultServices or optionalServices.\nThe way this scenario manifests is lifecycle will deploy s-X, s-A, and s-B, but the build will likely fail because s-B is missing a required dependency s-C.","solutions#Solutions":"There are 2 ways to address this depending on your use case.","solution-1#Solution 1":"Add s-B to r-B‚Äôs environment.defaultServices block in r-B.lifecycle.yaml. In effect, this will make s-C a first-level dependency.","solution-2#Solution 2":"Add s-C to the services.requires block of r-A in r-A.lifecycle.yaml. This will also make s-C a first-level dependency.","choosing-the-right-solution#Choosing the Right Solution":"In summary, the solution you should use depends on how you want your service to be consumed in an outside repo*.\nIf you want outside repos to explicitly include s-A and s-B, use Solution 1.\nIf you want outside repos to only include s-A and let dependencies resolve automatically, use Solution 2.","terminology#Terminology":"Service repo: The repository where lifecycle.yaml is defined.\nOutside repo: Another repository referencing it.\ndev-0: Default static environment."}},"/docs/features/template-variables":{"title":"Template Variables","data":{"overview#Overview":"Lifecycle uses Mustache as the template rendering engine.","available-template-variables#Available Template Variables":"The following template variables are available for use within your configuration. Variables related to specific services should use the service name as a prefix.","general-variables#General Variables":"{{{buildUUID}}} - The unique identifier for the Lifecycle environment, e.g., lively-down-881123.\n{{{namespace}}} - Namespace for the deployments, e.g., env-lively-down-881123.\n{{{pullRequestNumber}}} - The GitHub pull request number associated with the environment.","service-specific-variables#Service-Specific Variables":"For service-specific variables, replace <service_name> with the actual service name.\n{{{<service_name>_internalHostname}}} - The internal hostname of the deployed service. If the service is optional and not deployed, it falls back to defaultInternalHostname.\nservice_internalHostname will be substituted with local cluster full\ndomain name like service.namespace.svc.cluster.local to be able to work\nwith deployments across namespaces.\n{{{<service_name>_publicUrl}}} - The public URL of the deployed service. If optional and not deployed, it defaults to defaultPublicUrl under the services table.\n{{{<service_name>_sha}}} - The GitHub SHA that triggered the Lifecycle build.\n{{{<service_name>_branchName}}} - The branch name of the pull request that deployed the environment.\n{{{<service_name>_UUID}}} - The build UUID of the service. If listed under optionalServices or defaultServices, its value depends on whether the service is selected:\nIf selected, it is equal to buildUUID.\nIf not selected (or if service not part of deploys created), it defaults to dev-0.","usage-example#Usage Example":"This ensures the PUBLIC_URL and INTERNAL_HOST variables are dynamically assigned based on the ephemeral environment deployment.\nUndefined variables will result in an empty string unless handled explicitly.\nUse triple curly braces ({{{ }}}) to prevent unwanted HTML escaping.\nEnsure service names are correctly referenced in the template without any spaces.\nFor more details, refer to the Mustache.js documentation."}},"/docs/features/webhooks":{"title":"Webhooks","data":{"":"Lifecycle can invoke third-party services when a build state changes.Webhooks allow users to automate external processes such as running tests, performing cleanup tasks, or sending notifications based on environment build states.","supported-types#Supported Types":"Lifecycle supports three types of webhooks:\ncodefresh - Trigger Codefresh pipelines\ndocker - Execute Docker images as Kubernetes jobs\ncommand - Run shell commands in a specified Docker image","common-use-cases#Common Use Cases":"When a build status is deployed, trigger end-to-end tests.\nWhen a build status is error, trigger infrastructure cleanup or alert the team.\nRun security scans on built containers.\nExecute database migrations after deployment.\nSend notifications to Slack, Discord, or other communication channels.\nPerform smoke tests using custom test containers.","configuration#Configuration":"Webhooks are defined in the lifecycle.yaml under the environment.webhooks section.Below is an example configuration for triggering end-to-end tests when the deployed state is reached.","examples#Examples":"","codefresh#codefresh":"The codefresh type triggers existing Codefresh pipelines when build states change.\nstate: deployed ‚Üí Triggers the webhook when the build reaches the deployed state.\ntype: codefresh ‚Üí Specifies that this webhook triggers a Codefresh pipeline.\nname ‚Üí A human-readable name for the webhook.\npipelineId ‚Üí The unique Codefresh pipeline ID.\ntrigger ‚Üí Codefresh pipeline's trigger to execute.\nenv ‚Üí Passes relevant environment variables (e.g., branch and TEST_URL).\nstate: error ‚Üí Triggers the webhook when the build fails.\ntype: codefresh ‚Üí Invokes a Codefresh cleanup pipeline.\ntrigger: cleanup ‚Üí Codefresh pipeline's trigger to execute.\nenv ‚Üí Includes necessary variables, such as branch and CLEANUP_TARGET.","docker#docker":"The docker type allows you to execute any Docker image as a Kubernetes job when build states change.\nDocker webhooks run as Kubernetes jobs in the same namespace as your build.\nThey have a default timeout of 30 minutes and resource limits of 200m CPU and\n1Gi memory.\ndocker.image ‚Üí Docker image to execute (required)\ndocker.command ‚Üí Override the default entrypoint (optional)\ndocker.args ‚Üí Arguments to pass to the command (optional)\ndocker.timeout ‚Üí Maximum execution time in seconds (optional, default: 1800)","command#command":"The command type is a simplified version of Docker webhooks, ideal for running shell scripts or simple commands.\nMake sure to replace placeholder values like webhook URLs and pipeline IDs\nwith your actual values.\ncommand.image ‚Üí Docker image to run the script in (required)\ncommand.script ‚Üí Shell script to execute (required)\ncommand.timeout ‚Üí Maximum execution time in seconds (optional, default: 1800)","trigger-states#Trigger states":"Webhooks can be triggered on the following build states:\ndeployed ‚Üí Service successfully deployed and running\nerror ‚Üí Build or deployment failed\ntorn_down ‚Üí Environment has been destroyed","note#Note":"All webhooks for the same state are executed serially in the order defined.\nWebhook failures do not affect the build status.\nWebhook invocations can be viewed at /builds/[uuid]/webhooks page(latest 20 invocations). Use the API to view all invocations.\ndocker and command type's logs are not streamed when the job is still in progress and are available only after the job completes."}},"/docs/getting-started/configure-environment":{"title":"Configure environment","data":{"":"Now that we've created and deployed our first Lifecycle environment, let's learn how to customize it by configuring services and dependencies.","understanding-configuration#Understanding Configuration":"First, let's take a look at the lifecycle.yaml configuration file at the root dir of lifecycle-examples repository:","default-and-optional-services#Default and Optional Services":"We have our dependencies defined in defaultServices and optionalServices:\ndefaultServices ‚Äì These services are always built and deployed with the environment. They form the core foundation of the environment and are required for it to function correctly.\noptionalServices ‚Äì These services can be built on demand, only when explicitly needed. If they are not selected during a PR, they default to using a static environment (e.g., dev-0).","template-variables#Template Variables":"Notice how there are template variables defined in service named frontend > github.docker.env:\nThis API_URL and CACHE_URL variables are dynamically templated by Lifecycle and provided during the build and deploy steps for the frontend service.\nRead more about supported template variables\nhere","static-environment-as-a-fallback#Static Environment as a Fallback":"Since cache is an optional service, this service defaulted to using a static environment(dev-0) as a fallback. This allows us to reuse existing environments instead of rebuilding everything from scratch when there are no changes.","check-template-variables#Check Template Variables":"To view how the fallback URL works,\nOpen your Tasks App(frontend) from the deployed environment.\nNavigate to the Variables page.\nSearch for _URL and check its value.\nIt should look like:\nNotice how CACHE_URL defaults to the dev-0(static) environment for the optional cache.","configuring-services#Configuring Services":"Now, let's say you also want to the cache component to test, build and deploy it in your environment.","enable-cache-deployment#Enable Cache Deployment":"Navigate to the Lifecycle PR comment on GitHub.\nSelect the cache checkbox in the comment. That's it!\nLifecycle will now start building and deploying the cache service for your specific environment.\nWait for the build to complete. You can monitor the progress in the status comment.","confirm-the-new-cache-url#Confirm the New Cache URL":"Once the cache is deployed, go back to your frontend app‚Äôs Variables page.\nCheck the CACHE_URL value.\nIt should now look like:\nNow, you're running your cache from your own environment instead of an existing static deploy!\nCheck the application‚Äôs Tasks page while you‚Äôre here and observe the completely different data, as this environment uses a freshly built and seeded database.","build-flexible-environments#Build Flexible Environments":"With this approach, you can:\nBuild any combination of frontend and backend services.\nUse custom branches for different services.\nTest different versions of your app.\nCheck how to use Mission Control comments for configuring your environment\nhere\nThis gives you a custom, isolated testing environment that mirrors your\nproduction setup while allowing flexibility in development and validation.","summary#Summary":"Services marked as optional in lifecycle.yaml will default to static environments unless explicitly built.\nYou can enable/disable any service directly from the Lifecycle PR comment.\nLifecycle automates dependency management, ensuring your services deploy in the correct order.\nNow you're ready to customize your Lifecycle environments like a pro! üë©‚Äçüíª"}},"/docs/getting-started/create-environment":{"title":"Create environment","data":{"":"In this walk through, we will make a simple change to an example frontend repository and create our first ephemeral environment using Lifecycle.","1-fork-the-repository#1. Fork the Repository":"Fork the lifecycle-examples repository to your org or personal account and install your newly minted GitHub App to the forked repository.\nNavigate to https://github.com/settings/apps (for personal accounts) or https://github.com/organizations/<org>/settings/apps (for org accounts).\nFind the Lifecycle GitHub App and click on Edit.\nChoose Install App from sidebar and click the Settings  icon.\nSelect the forked repository from the list and select Save.","2-create-a-new-branch#2. Create a New Branch":"Clone the repo and create a branch named lfc-config:\nor if you are using GitHub Desktop, you can create a new branch from the UI.","3-update-lifecycle-configuration#3. Update Lifecycle Configuration":"Open the lifecycle.yaml file in the root of the repository and update the frontend service's repository to your github username or org.Before:\nAfter:","4-commit--push-your-changes#4. Commit & Push Your Changes":"","5-create-a-pull-request#5. Create a Pull Request":"Open a Pull Request (PR) from lfc-config to main in the forked repository.\nSubmit the PR.","6-lifecycle-pr-comment#6. Lifecycle PR Comment":"After submitting the PR, you‚Äôll see a GitHub comment from Lifecycle on your pull request.üîπ This PR comment is the mission control for your ephemeral environment. It provides:\nA status update of the build and deploy process.\nA list of services configured for the environment.\nA link to the Lifecycle UI where you can view logs, deployments, and environment details.\nIf there is no comment from Lifecycle, it means the app is not configured\ncorrectly or the GitHub App is not installed in the repository. Please refer\nto the Missing Comment page for\nmore information.","7-add-lifecycle-status-comments-label#7. Add lifecycle-status-comments! label":"The additional label lifecycle-status-comments! provides more detailed information about the environment status and links to access the running application.üîπ The comments provides insights into:\nBuild & Deploy Status: Track when your environment is ready.\nEnvironment URLs: Access the running frontend app.\nTelemetry Links: Links to telemetry, build and deploy logs. (if enabled)","8-wait-for-deployment#8. Wait for Deployment":"Wait for the builds & deploys to complete. Once the status updates to deployed, your environment is live! üöÄWhen a new commit is pushed to your pull request Lifecycle automatically builds and deploys again so you always have the latest version of the application.\nIf there are any errors during the build or deploy process, the environment\nwill not be created, and you will see an error message in the Lifecycle\ncomment.\nYou can check the logs from lifecycle-worker pods in your cluster to debug\nthe issue:  kubectl logs deploy/lifecycle-worker -n lifecycle-app -f","9-checkout-the-deployed-application#9. Checkout the deployed application":"Once the deployment is complete, you can access your environment at the URL provided in the Lifecycle comment on your pull request. Click on the frontend link to open your application in a new tab.The application has two simple pages:\n/tasks ‚Äì A simple to-do list.\n/variables ‚Äì Displays all environment variables from the container.","next-steps#Next Steps":"Now that your first ephemeral environment is ready, move on to the next section where we:üß™ Test the environment.\nüß≠ Explore the comments and logs.\n‚öôÔ∏è Customize the configuration."}},"/docs/getting-started/explore-environment":{"title":"Explore environment","data":{"":"Now that we've deployed our first Lifecycle environment, let‚Äôs take a tour of the PR comments to understand how to interact with our ephemeral environment.","test-your-application#Test Your Application":"Let's navigate to the deployed frontend app from the PR comment.\nClick on the frontend link in the PR comment to navigate to your deployed application.\nAdd a task and complete few tasks to update data in backend.\nNavigate to the variables page and checkout the variables in your application's container.\nThats it! You have successfully deployed and tested the best todo app in the world! üéâ","mission-control-comment#Mission Control Comment":"The Lifecycle PR comment in your pull request serves as the mission control for your ephemeral environment.","what-you-can-do-in-the-pr-comment#What You Can Do in the PR Comment":"Editable Checkboxes: Select or deselect services to include in your environment.\nRedeploy Checkbox: Triggers a redeploy (useful for transient issues).\nDeployment Section: Provides URLs to your deployed services.\nRead more about Mission Control comment\nhere","status-comment#Status Comment":"When we add the lifecycle-status-comments! label to our pull request, Lifecycle will automatically add a status comment to the PR.This comment provides real-time updates on the status, links to your deployments including the build progress and service statuses.\nNotice the following while the environment is being built:\nThe status comment is updated in real-time.\nThe status of each service is displayed.\nThe build logs are available for each service.","next-steps#Next Steps":"In the next section, we will:‚öôÔ∏è Customize our configuration\n‚òëÔ∏è Enable and build an optional service(cache) support your applicationReady to level up your ephemeral environment? Let's go! üèÉ‚Äç‚û°Ô∏è"}},"/docs/getting-started/delete-environment":{"title":"Delete environment","data":{"":"To tear down an environment, you can do one of the following:\nMerge or close the pull request: This will automatically clean up the environment.\nApply the lifecycle-disabled! label: This will immediately trigger the environment deletion process.\nThe lifecycle-disabled! label is useful in scenarios where:\nThe environment infrastructure is experiencing issues.\nThe data within the environment is corrupt.\nYou need to restart or rebuild the environment from scratch without waiting for a PR to be merged or closed.\nSimply apply the label to the PR associated with the environment, and Lifecycle will automatically tear it down.\nRead more about how pull request labels control auto deploy in repositories\nhere\nUsing these methods, you can efficiently manage and clean up environments to ensure smooth development and testing workflows. üßπ"}},"/docs/getting-started/explore-static-environment":{"title":"Explore static environment","data":{"":"A static environment in Lifecycle is a persistent environment that serves as a fallback when dependent services do not need to be rebuilt.Unlike ephemeral environments that are built on short lived pull requests, static environments are built on top of long lived pull requests. These environments exist continuously and update automatically as changes are merged into the default branch of configured services.","what-is-dev-0#What is dev-0":"The default static environment is dev-0. This environment ensures that there is always a stable and up-to-date version of services available without needing to build every dependency manually.\nThe dev-0 environment should be created for your installation.During the initial bootstrapping of Lifecycle, the dev-0 build record is created automatically but this itself does not have any services built.","create-dev-0#Create dev-0":"Delete the dummy dev-0 build record from builds table in the database\nCreate a repository named lifecycle-static-env in your GitHub account\nInstall the Lifecycle GitHub App in this repository\nCreate a pull request in this repository with branch dev-0\nAdd lifecycle.yaml file to the root of the repository with all the services you want to include in the dev-0 environmentExample:\nDeploy the dev-0 environment by adding lifecycle-deploy! label to the pull request\nUpdate uuid for the environment to dev-0 in the mission control comment\nFinally, execute this query to track default branches of the services in the dev-0 environment:","key-features#Key Features":"üèóÔ∏è Fallback for Optional Services\nWhen optional services are not explicitly built in an ephemeral environment, Lifecycle defaults to using the latest build from dev-0.\nüí™ Based on a Persistent PR\nSimilar to ephemeral environments, dev-0 is based on a PR, but it remains open and continuously updates.\nüë£ Tracks Changes on Default Branch Merges\nWhenever a service has a new change merged to its main branch, dev-0 will automatically pull, build, and redeploy the latest changes.\nThis ensures dev-0 always contains the freshest version of all services."}},"/docs/getting-started/terminology":{"title":"Terminology","data":{"":"This glossary provides an overview of key Lifecycle concepts and terminology. Let's see how they fit into the environment setup and deployment process.","repository#Repository":"A repository refers to a GitHub repository. Each environment that is built must have a default repository and an associated pull request.","service#Service":"A service is a deployable artifact. It can be a Docker container, CI pipeline, RDS database, or Helm chart. A single repository can contain multiple services.Example: \nfrontend-service and frontend-cache are two services required for the frontend application to function correctly.","environment#Environment":"An environment is a stack of services built and connected together.\ndefaultServices are built and deployed in an environment by default.\noptionalServices can be built and deployed only when needed; otherwise, they fallback to the default static environment.","static-environment#Static Environment":"A static environment is a long-lived environment based on a pull request. It tracks branches from configured services and updates automatically when new changes are merged.","build#Build":"A build is the actual instance of the process to build and deploy services within an environment.\nEach build is uniquely identified by Lifecycle using a UUID (e.g., arm-model-060825 or dev-0).\nA build contains one deploy per service in the configuration.","deploy#Deploy":"A deploy manages the build and deployment execution of a service within an environment.Example:\nIn a frontend environment, frontend-service and frontend-cache are two deploys created for the environment, each mapped to a unique build UUID.","webhook#Webhook":"Lifecycle can invoke third-party services when a build state changes. Currently, only Codefresh triggers are supported.","example#Example":"When the build status is deployed, trigger end-to-end tests.\nWhen the build status is error, trigger infrastructure cleanup."}},"/docs/schema/codefresh":{"title":"Codefresh Service","data":{"":"The codefresh service type delegates deployment to external Codefresh pipelines. This is useful when you have existing CI/CD pipelines that handle complex build and deployment logic.","example#Example":"","required-fields#Required Fields":"Field\tType\tDescription\trepository\tstring\tGitHub repository in owner/repo format\tbranchName\tstring\tBranch to build from\tdeploy.pipelineId\tstring\tCodefresh pipeline ID for deployment\tdeploy.trigger\tstring\tPipeline trigger name for deployment\tdestroy.pipelineId\tstring\tCodefresh pipeline ID for destruction\tdestroy.trigger\tstring\tPipeline trigger name for destruction","optional-fields#Optional Fields":"Field\tType\tDescription\tenv\tobject\tEnvironment variables passed to pipelines","pipeline-configuration#Pipeline Configuration":"","deploy#deploy":"Configure the pipeline triggered when deploying the service.","destroy#destroy":"Configure the pipeline triggered when tearing down the environment.","environment-variables#Environment Variables":"Pass environment variables to both deploy and destroy pipelines:\nEnvironment variables defined in env are passed to both deploy and destroy\npipelines. Use template variables to reference other services.","how-it-works#How It Works":"When an environment is deployed, Lifecycle triggers the deploy pipeline with the configured environment variables.\nThe pipeline receives context about the deployment including branch, namespace, and service information.\nWhen the environment is torn down, Lifecycle triggers the destroy pipeline to clean up resources.","use-cases#Use Cases":"Complex build processes - When builds require multi-stage pipelines or custom tooling\nExternal dependencies - When deployment requires interaction with external systems\nExisting CI/CD - When you have established Codefresh pipelines you want to reuse\nCustom deployment logic - When standard Kubernetes deployment isn't sufficient\nCodefresh pipelines run asynchronously. Lifecycle tracks pipeline status but\nthe actual deployment logic is managed by Codefresh.","finding-pipeline-ids#Finding Pipeline IDs":"To find your Codefresh pipeline ID:\nOpen your pipeline in Codefresh\nThe pipeline ID is in the URL: https://g.codefresh.io/pipelines/edit/new/.../<pipeline-id>\nOr use the Codefresh CLI: codefresh get pipelines"}},"/docs/schema/aurora-restore":{"title":"Aurora Restore Service","data":{"":"The auroraRestore service type restores AWS Aurora database snapshots for use in ephemeral environments. This allows you to test against production-like data.\nThis documentation is still in progress and will be updated shortly with the\nlatest information.","required-fields#Required Fields":"Field\tType\tDescription\tcommand\tstring\tCommand to execute for the restore operation\targuments\tstring\tArguments passed to the command","how-it-works#How It Works":"When the environment is deployed, Lifecycle executes the configured command with arguments\nThe command typically triggers an Aurora snapshot restore process\nOnce complete, the restored database is available for other services\nThe auroraRestore service type is a specialized service that runs a command\nto initiate database restoration. The actual restore logic is implemented in\nyour command/script.","example#Example":"","use-cases#Use Cases":"Production data testing - Test features against realistic production data\nDatabase migrations - Validate migrations against production schema\nPerformance testing - Benchmark against production-scale data\nBug reproduction - Reproduce issues with production data state\nWhen restoring from production snapshots, ensure sensitive data is properly\nanonymized or that access is appropriately restricted.","implementation-notes#Implementation Notes":"The restore command is executed as part of the environment deployment process. Your script should:\nHandle authentication with AWS\nInitiate the snapshot restore\nWait for the restore to complete\nConfigure network access to the restored instance\nReturn the connection information\nThe restored database endpoint can then be referenced by other services using template variables."}},"/docs/schema/configuration":{"title":"Configuration Service","data":{"":"The configuration service type creates a configuration-only service that doesn't deploy any containers. It's useful for managing feature flags, shared configuration, or service metadata that other services can reference.\nThis documentation is still in progress and will be updated shortly with the\nlatest information.","required-fields#Required Fields":"Field\tType\tDescription\tdefaultTag\tstring\tConfiguration version tag\tbranchName\tstring\tBranch name for configuration source","how-it-works#How It Works":"A configuration service:\nDoes not deploy any containers or pods\nRegisters a service entry in Lifecycle for reference by other services\nProvides a way to version and track configuration changes\nCan be referenced by other services using template variables","example#Example":"","use-cases#Use Cases":"","feature-flags#Feature Flags":"Track feature flag configuration versions:","shared-configuration#Shared Configuration":"Reference shared configuration across services:","environment-metadata#Environment Metadata":"Track environment-specific metadata:\nConfiguration services are lightweight entries in Lifecycle's service\nregistry. They don't consume cluster resources but allow other services to\nreference configuration state."}},"/docs/schema/docker":{"title":"Docker Service","data":{"":"The docker service type deploys pre-built Docker images without building from source. This is ideal for databases, caches, message queues, and other infrastructure components.","examples#Examples":"PostgreSQL database (internal only):\nReference from other services:\nRedis cache:\nReference from other services:\nMySQL with persistent storage:\nComplete configuration with all options:","fields-reference#Fields Reference":"","required-fields#Required Fields":"Field\tType\tDescription\tdockerImage\tstring\tDocker image name (e.g., postgres, redis)\tdefaultTag\tstring\tImage tag (e.g., 15-alpine, 7-alpine)","optional-fields#Optional Fields":"Field\tType\tDescription\tcommand\tstring\tOverride container entrypoint\targuments\tstring\tArguments passed to the command. Use %%SPLIT%% for spaces\tenv\tobject\tEnvironment variables\tports\tarray\tExposed container ports\tdeployment\tobject\tDeployment configuration (see below)\tenvLens\tboolean\tEnable environment lens ingress banner","deployment-options#Deployment Options":"The deployment section configures how the service is deployed to Kubernetes. All fields are optional.See GitHub Service - Deployment Options for detailed documentation of each field.\nField\tDescription\tpublic\ttrue = creates ingress, false = internal only\tcapacityType\tNode type: spot or on-demand\tresource\tCPU and memory requests/limits\treadiness\tHealth check using httpGet or tcpSocketPort\thostnames\tCustom hostname config (auto-constructed from global_config if omitted)\tnetwork\tIP whitelist, port mapping, gRPC support\tserviceDisks\tPersistent volume mounts\t\nFor databases and caches, you typically want public: false to keep them\ninternal to the cluster.","common-images#Common Images":"","databases#Databases":"","caches--queues#Caches & Queues":""}},"/docs/schema/environment":{"title":"Environment Configuration","data":{"":"The environment section controls how Lifecycle deploys and manages your ephemeral environments. It defines which services are deployed by default, which are optional, and how automation (webhooks) should behave.","fields-reference#Fields Reference":"Field\tType\tRequired\tDescription\tautoDeploy\tboolean\tNo\tAutomatically deploy on PR updates\tdefaultServices\tarray\tYes\tServices deployed with every environment\toptionalServices\tarray\tNo\tServices available on-demand\twebhooks\tarray\tNo\tAutomation triggers on state changes\tenabledFeatures\tarray\tNo\tFeature flags for the environment\tgithubDeployments\tboolean\tNo\tCreate GitHub deployment records\tuseGithubStatusComment\tboolean\tNo\tPost status comments on PRs","defaultservices#defaultServices":"Services listed in defaultServices are deployed automatically when an environment is created. At least one default service is required.","service-reference-fields#Service Reference Fields":"Each entry in defaultServices can include optional fields to override the service configuration:\nField\tType\tDescription\tname\tstring\tRequired. Name matching a service in the services array\trepository\tstring\tOverride the repository for this service\tbranch\tstring\tOverride the branch for this service\tserviceId\tnumber\tInternal service ID reference","optionalservices#optionalServices":"Services listed in optionalServices are not deployed by default but can be added to an environment on-demand through the Lifecycle UI or API.\nOptional services are useful for components that aren't always needed, like\nbackground workers, caching layers, or additional databases for specific\ntesting scenarios.","autodeploy#autoDeploy":"When autoDeploy is enabled, Lifecycle automatically triggers a deployment whenever the PR is updated (new commits pushed).\nWith autoDeploy: true, environments are created automatically on every\ncommit to the PR branch without requiring manual label addition.","webhooks#webhooks":"Webhooks allow you to trigger external actions when the environment reaches certain states. See the Webhooks page for detailed configuration.","examples#Examples":"The minimal configuration requires only defaultServices:\nConfiguration with both default and optional services:\nComplete configuration with all options:"}},"/docs/schema/github":{"title":"GitHub Service","data":{"":"The github service type builds and deploys applications from a GitHub repository using a Dockerfile. This is the most common service type for application code that needs to be built from source.","examples#Examples":"Minimal configuration with only required fields:\nConfiguration with deployment options:\nComplete configuration with all options:","docker-configuration#Docker Configuration":"The docker section defines how the application is built and run.","dockerdefaulttag#docker.defaultTag":"The default Docker image tag, typically matching the branch name.","dockerbuilder#docker.builder":"Configuration for the Docker build process. The engine field specifies which build engine to use:\nbuildkit - BuildKit engine (default)\ncodefresh - Codefresh build engine\nkaniko - Kaniko build engine","dockerapp-required#docker.app (Required)":"Configuration for the main application container:\ndockerfilePath - Required. Path to Dockerfile relative to repo root\ncommand - Override container entrypoint\narguments - Arguments passed to the command. Use %%SPLIT%% as a delimiter for spaces (e.g., -c%%SPLIT%%npm run start)\nenv - Environment variables\nports - Exposed container ports","dockerinit-optional#docker.init (Optional)":"Configuration for an init container that runs before the main application. Uses the same fields as docker.app.","deployment-options#Deployment Options":"The deployment section configures how the service is deployed to Kubernetes. All fields are optional.","public#public":"Controls whether the service is exposed via ingress for external access.\ntrue - Creates an ingress, service gets a public URL\nfalse - Internal only, accessible only within the cluster","capacitytype#capacityType":"Specifies the node capacity type for scheduling:\nspot - Use spot/preemptible instances (cost-effective)\non-demand - Use on-demand instances (more reliable)","resource#resource":"CPU and memory requests and limits for the container:\nField\tDescription\tcpu.request\tMinimum CPU guaranteed (e.g., 100m)\tcpu.limit\tMaximum CPU allowed (e.g., 1000m)\tmemory.request\tMinimum memory guaranteed (e.g., 256Mi)\tmemory.limit\tMaximum memory allowed (e.g., 1Gi)","readiness#readiness":"Health check configuration to determine when the service is ready to receive traffic.HTTP health check:\nField\tDescription\thttpGet.path\tHTTP endpoint path (e.g., /health)\thttpGet.port\tPort to check\t\nTCP health check:\nField\tDescription\ttcpSocketPort\tTCP port to check connectivity\t\nCommon fields:\nField\tDescription\tinitialDelaySeconds\tDelay before first check\tperiodSeconds\tInterval between checks\ttimeoutSeconds\tTimeout for each check\tsuccessThreshold\tConsecutive successes to be healthy\tfailureThreshold\tConsecutive failures to be unhealthy","hostnames#hostnames":"Custom hostname configuration. If omitted, hostnames are auto-constructed from global_config values.\nField\tDescription\thost\tCustom hostname suffix\tdefaultInternalHostname\tInternal Kubernetes hostname\tdefaultPublicUrl\tDefault public URL\tacmARN\tAWS ACM certificate ARN for TLS","network#network":"Advanced network configuration:\nField\tDescription\tipWhitelist\tArray of allowed IP ranges (CIDR notation)\tpathPortMapping\tMap URL paths to container ports\thostPortMapping\tMap hostnames to container ports\tgrpc.enable\tEnable gRPC support\tgrpc.host\tgRPC hostname","servicedisks#serviceDisks":"Persistent volume mounts for stateful data:\nField\tRequired\tDescription\tname\tYes\tVolume name\tmountPath\tYes\tPath inside the container\tstorageSize\tYes\tStorage size (e.g., 10Gi)\taccessModes\tNo\tReadWriteOnce or ReadWriteMany\tmedium\tNo\tStorage medium","fields-reference#Fields Reference":"","required-fields#Required Fields":"Field\tType\tDescription\trepository\tstring\tGitHub repository in owner/repo format\tbranchName\tstring\tBranch to build from\tdocker\tobject\tDocker build configuration (see above)","optional-fields#Optional Fields":"Field\tType\tDescription\tdeployment\tobject\tDeployment configuration (see above)\tenvLens\tboolean\tEnable environment lens ingress banner","template-variables#Template Variables":"Reference other services in your configuration using template variables. See the Template Variables guide for the complete list."}},"/docs/schema/webhooks":{"title":"Webhooks Configuration","data":{"":"Webhooks allow you to trigger automated actions when an environment reaches certain states. They are configured under environment.webhooks in your lifecycle.yaml.\nFor detailed use cases, examples, and execution behavior, see the\nWebhooks guide.","common-fields#Common Fields":"All webhook types share these common fields:\nField\tType\tRequired\tDescription\tname\tstring\tNo\tHuman-readable name for the webhook\tdescription\tstring\tNo\tDescription of what the webhook does\tstate\tstring\tYes\tTrigger state: deployed, error, or torn_down\ttype\tstring\tYes\tWebhook type: codefresh, docker, or command\tenv\tobject\tYes\tEnvironment variables passed to the webhook","trigger-states#Trigger States":"State\tDescription\tdeployed\tAll services successfully deployed and running\terror\tBuild or deployment failed\ttorn_down\tEnvironment has been destroyed","webhook-types#Webhook Types":"","codefresh#codefresh":"Triggers an existing Codefresh pipeline.\nField\tType\tRequired\tDescription\tpipelineId\tstring\tYes\tCodefresh pipeline ID\ttrigger\tstring\tNo\tPipeline trigger name","docker#docker":"Runs a Docker image as a Kubernetes job.\nField\tType\tRequired\tDescription\tdocker.image\tstring\tYes\tDocker image to execute\tdocker.command\tarray\tNo\tOverride entrypoint command\tdocker.args\tarray\tNo\tArguments to pass to the command\tdocker.timeout\tnumber\tNo\tMax execution time in seconds (default: 1800)","command#command":"Runs a shell script in a specified Docker image.\nField\tType\tRequired\tDescription\tcommand.image\tstring\tYes\tDocker image to run the script in\tcommand.script\tstring\tYes\tShell script to execute\tcommand.timeout\tnumber\tNo\tMax execution time in seconds (default: 1800)","template-variables#Template Variables":"You can use template variables like {{api_publicUrl}} in the env section to reference dynamic values from your services.See the Template Variables guide for the complete list of available variables."}},"/docs/schema/helm":{"title":"Helm Service","data":{"":"The helm service type deploys applications using Helm charts. It supports local charts, OCI registries, and public Helm repositories. You can optionally include a Docker build step for custom images.\nFor advanced Helm deployment features like native Helm and environment\nvariable mapping, see the Native Helm\nDeployment guide.","examples#Examples":"Local chart with Docker build:\nOCI registry chart with Docker build:\nFor Helm template variables that should be resolved at deployment time, use triple braces {{{variable}}} to prevent Helm from processing them prematurely.\nPublic Bitnami chart (no Docker build):\nOther popular Bitnami charts:\nComplete configuration with all options:","chart-configuration#Chart Configuration":"The chart section is required and defines which Helm chart to deploy.\nCommonly used charts can be configured in the global_config table to reuse\nchart configuration across multiple services.","chartname-required#chart.name (Required)":"The chart name can be one of:\nValue\tDescription\t\"./\" or \"../\" prefix\tRelative path to local chart\t\"oci://...\"\tOCI registry chart URL\tChart name\tPublic chart name from a repository","chartrepourl#chart.repoUrl":"URL of the Helm repository (required for public charts).","chartversion#chart.version":"Specific chart version to deploy.","chartvalues#chart.values":"Array of Helm values in key=value format.","chartvaluefiles#chart.valueFiles":"Array of value file paths relative to the repository root.","docker-configuration#Docker Configuration":"The optional docker section defines how to build a custom image. This uses the same configuration as the GitHub service docker section.","dockerdefaulttag#docker.defaultTag":"The default Docker image tag, typically matching the branch name.","dockerbuilder#docker.builder":"Configuration for the Docker build process. The engine field specifies which build engine to use:\nbuildkit - BuildKit engine (default)\ncodefresh - Codefresh build engine\nkaniko - Kaniko build engine","dockerapp-required-when-using-docker#docker.app (Required when using docker)":"Configuration for the main application container:\ndockerfilePath - Required. Path to Dockerfile relative to repo root\ncommand - Override container entrypoint\narguments - Arguments passed to the command. Use %%SPLIT%% as a delimiter for spaces (e.g., -c%%SPLIT%%npm run start)\nenv - Environment variables\nports - Exposed container ports","dockerinit-optional#docker.init (Optional)":"Configuration for an init container that runs before the main application. Uses the same fields as docker.app.","fields-reference#Fields Reference":"Field\tType\tRequired\tDescription\tchart.name\tstring\tYes\tChart name or path\tchart.repoUrl\tstring\tFor public\tHelm repository URL\tchart.version\tstring\tNo\tChart version\tchart.values\tarray\tNo\tInline Helm values\tchart.valueFiles\tarray\tNo\tValue file paths\trepository\tstring\tFor builds\tGitHub repository\tbranchName\tstring\tFor builds\tBranch to build from\tdocker\tobject\tNo\tDocker build config\targs\tstring\tNo\tAdditional Helm arguments\tversion\tstring\tNo\tHelm CLI version\tdeploymentMethod\tstring\tNo\t\"native\" or \"ci\"\tenvLens\tboolean\tNo\tEnable environment lens ingress banner","templated-variables#Templated Variables":"Use templated variables in chart values to reference dynamic deployment values. See the Template Variables guide for the complete list.\nUse triple braces {{{variable}}} for Lifecycle template variables. This prevents Helm from trying to process them as Helm template functions.","native-helm-deployment#Native Helm Deployment":"For more control over Helm deployments, enable native Helm:\nNative Helm provides:\nDirect Kubernetes job execution\nReal-time deployment logs\nBetter handling of concurrent deployments\nFull Helm argument control\nSee the Native Helm Deployment guide for details."}},"/docs/schema/overview":{"title":"Schema Overview","data":{"":"The lifecycle.yaml file is the core configuration file that defines how Lifecycle manages your ephemeral environments. Place this file at the root of your repository.","file-structure#File Structure":"A lifecycle.yaml file has three main sections:\nSection\tDescription\tversion\tSchema version (currently \"1.0.0\")\tenvironment\tControls deployment behavior, service grouping, and webhooks\tservices\tArray of service definitions with their configurations","service-types#Service Types":"Each service in the services array must have exactly one service type configured. Choose based on your deployment needs:\nService Type\tUse Case\tgithub\tBuild and deploy from a GitHub repository with Dockerfile\tdocker\tDeploy pre-built Docker images (databases, caches, etc.)\thelm\tDeploy using Helm charts (local or remote)\tcodefresh\tTrigger external Codefresh pipelines for deployment\tauroraRestore\tRestore AWS Aurora database snapshots\tconfiguration\tDeploy configuration-only services (feature flags, shared config)","choosing-a-service-type#Choosing a Service Type":"","minimal-example#Minimal Example":"Here's the simplest valid lifecycle.yaml for a single service:","complete-example#Complete Example":"A more complete example with multiple service types:","template-variables#Template Variables":"Throughout your configuration, you can use template variables like {{api_publicUrl}} or {{database_internalHostname}} to reference dynamic values from other services.See the Template Variables guide for the complete list of available variables and usage examples.","next-steps#Next Steps":"Environment Configuration - Configure deployment behavior and service grouping\nWebhooks - Automate actions on deployment events\nGitHub Service - Build and deploy from source\nDocker Service - Deploy pre-built images\nHelm Service - Deploy with Helm charts"}},"/docs/setup/configure-lifecycle":{"title":"Additional Configuration","data":{"":"We are in the final step of the setup process.This step is Optional but highly recommended to ensure the default IP Whitelist is set for the environments created by the Lifecycle app. This will help in securing the environments and restricting access to only the specified IPs or CIDR blocks.","set-default-ip-whitelist#Set Default IP Whitelist":"Connect to the postgres database using the psql command line tool or any other database client.\nDatabase password was auto generated during the infra setup and can be found\nretrieved from the app-postgres secret in the lifecycle-app\nnamespace.\nRetrieve the database password:\nRun the following SQL commands to update the configuration:\nNote that the infra setup with the OpenTofu modules below will open your\ncluster to the world. \nüõ°Ô∏è Make sure to shield your cluster by implementing appropriate network policies\nand access controls after the initial setup.Replace the defaultIPWhiteList under global_config.serviceDefaults with your actual IP whitelist or CIDR block to restrict access to the deployed environments.","refresh-config-cache#Refresh config cache":"This will refresh the configuration cache and apply the changes you made to the database for the Lifecycle app.We are all set! üéâ And ready to create our first PR based ephemeral environment."}},"/docs/setup/create-github-app":{"title":"Configure Application","data":{"configure-buildkit-endpoint#Configure BuildKit Endpoint":"Before creating the GitHub app, you need to configure the BuildKit endpoint in the database:\nSet the HELM_RELEASE environment variable to your actual Helm release name\nbefore running the commands below.\nThe following commands will create the buildkit object and endpoint\nconfiguration if they don't exist, or update them if they do.","option-1-using-kubectl-exec-with-psql#Option 1: Using kubectl exec with psql":"Execute the following commands to connect to the PostgreSQL pod and run the query:","option-2-direct-sql-query#Option 2: Direct SQL query":"If you have direct database access, run the following SQL query (replace <YOUR-HELM-RELEASE> with your actual Helm release name):","refresh-configuration-cache#Refresh Configuration Cache":"After running either option above, refresh the configuration cache:\nReplace <your_domain> with your actual domain (e.g., 0env.com).","create-github-app#Create GitHub App":"To create a Github app that will send events to the Lifecycle with necessary permissions, follow these steps:\nMake sure you have admin access to the Github organization or account where\nyou want to create the app.\nNavigate to your installed Lifecycle app at https://app.<your_domain>/setup (replace <your_domain> with your actual domain. e.g. https://app.0env.com/setup).\nSelect Personal or Organization based on your needs.\nFill in the required fields:\nGithub App Name: A name for your app. (should be unique, use a prefix with your name or organization. Refer Github app naming convention here\nOrganization Name: Github organization name where the app will be created. Required if you selected Organization.\nClick Create App\nOn the Github app creation page, confirm the app name and click Create\nOnce the app is created, you will be redirected to the app installation page where you can choose one or more repositories to install the the newly minted app.\nMake sure to select the repositories you want the app to have access to. You\ncan always change this later in the app settings but adding atleast one\nrepository is required to proceed with the setup.\nVoila! üéâ Your Github app is now created and installed.\nClick Configure and Restart to apply the changes and start using the app.\nThe step above, sets up the global config values that Lifecycle app will use\ncreating ephemeral environments and processing pull requests. And restarts the\ndeployment for the github app secrets to take effect.\nLet's move on the final step where we will configure the Lifecycle app config for processing pull requests and creating ephemeral environments."}},"/docs/setup/install-lifecycle":{"title":"Install Lifecycle","data":{"":"Now that the infrastructure components are setup, let's install the lifecycle app and create a new Github app that will send events to the application to process and create ephemeral dev environments.\nMake sure you have updated the kube config to be able to helm install in the\ncluster you just created!\nFollow installation steps in lifecycle helm chart\nWait for the installation to complete and verify that the pods are running:\nOnce the pods are running, you can access the application at your configured domain (e.g. https://app.0env.com)\nJust like that, you have successfully installed Lifecycle and set up the necessary infrastructure to start creating ephemeral environments for your GitHub pull requests!If you notice any secure certificate issues when accessing the application, you can check the status of your certificate using the following command:\nMake sure the certificate is in the Ready state. If it is not, you may need to wait a bit longer for the certificate to be issued or troubleshoot any issues with your DNS settings.Let's move on to the next step where we will create a GitHub app to connect Lifecycle with your repositories."}},"/docs/setup/prerequisites":{"title":"Prerequisites","data":{"":"Before we start with the setup, let's make sure the following prerequisites are in place:\nGitHub Account: You'll need either a personal or an organization GitHub account. Sign up for GitHub\nCloud Provider Account: A Google Kubernetes Engine (GKE) or Amazon Web Services (AWS) Account. You'll need an active account with either platform to proceed.\nSign up for Google Cloud and create a project\nSign up for AWS\nWe recommend using an isolated project or account in your cloud provider\nspecifically for this setup to begin with. This helps to keep your resources\norganized and manageable as you experiment with Lifecycle.\nCLI Tools\nOpenTofu ‚Äî Infrastructure as code tool (OpenTofu is a community-driven fork of Terraform).\nkubectl ‚Äî Command-line tool for interacting with Kubernetes clusters.\ngcloud or aws-cli ‚Äî Command-line tools for managing Google Cloud or AWS resources, respectively.\nCustom Domain: You will need a custom domain (e.g., 0env.com) to route traffic to your application environments. This is particularly important for setting up:\nPublic callback and webhook URLs for the GitHub App\nIngress routing within the Kubernetes cluster\nSecure (HTTPS) access via TLS certificates\nDNS Provider with Wildcard Support: The domain must be managed by a DNS provider that supports wildcard DNS records (e.g., *.0env.com). This is necessary to dynamically route traffic from GitHub to the Lifecycle app and to ephemeral environments.Supported DNS providers that support wildcard for infrastructure setup include:\nManual Setup:\nSetup a public DNS zone in Google Cloud to manage your domain's DNS records.\nFollow steps here to setup a\npublic DNS zone.\nWildcard DNS records will be created by the OpenTofu modules in the next steps.\nCLI Setup:\nUse the gcloud CLI to create a public DNS zone for your domain:\nUpdate your domain's DNS records with NS records provided by Google Cloud DNS. You can find these in the Google Cloud Console under the DNS zone you created.\nAWS Route 53: Amazon's scalable DNS web\nservice designed to route end users to Internet applications.Manual Setup:\nAuthenticate with AWS CLI using the role/usr you desire.\nEnsure you have your domain provisioned to accept wildcards; eg *.lifecycle.<your-domain>.com\nCLI Setup:\nIf you want to use Cloudflare as your primary DNS provider and manage your DNS records on Cloudflare, your domain should be using a full setup.\nThis means that you are using Cloudflare for your authoritative DNS nameservers.\nFollow the steps here to setup a public DNS zone in Cloudflare.\nEnsure that your domain‚Äôs nameservers are pointing to your chosen DNS provider\nat your registrar, and that you have permission to create and manage DNS\nrecords programmatically.  This is crucial for the setup to work\ncorrectly and will take time to propagate.Use https://dnschecker.org/#NS to verify that your domain's nameservers are correctly set up.\nOnce you have these prerequisites in place, you can proceed to the next steps in setting up the cluster and application."}},"/docs/setup/setup-infra":{"title":"Setup your cluster","data":{"":"Based on the prerequisites you've set up, you're now ready to configure your Kubernetes cluster for Lifecycle. This setup will ensure that your cluster is properly configured to run Lifecycle and manage your application environments effectively.\nNote that the infra setup with the OpenTofu modules below will open your\ncluster to the world. \nüõ°Ô∏è Make sure to shield your cluster by implementing appropriate network policies\nand access controls after the initial setup.\nClick on the cloud provider you are using to set up your cluster:\nGoogle Cloud Platform (GCP)\nAmazon Web Services (AWS)","google-cloud-platform#Google Cloud Platform":"","setup-application-credentials#Setup application credentials":"Enable Kubernetes Engine and Cloud DNS APIs:\nNote that you need to replace <PROJECT_ID> with your actual Google Cloud project ID not the project name.","bootstrap-infrastructure#Bootstrap infrastructure":"Clone the infrastructure repository:\nFollow steps in the infrastructure repository to set up the necessary infrastructure components.\nExample secrets.auto.tfvars file:\nInitialize and apply the Terraform configuration:\nThis will create the necessary infrastructure components, including the Kubernetes cluster, DNS records, database, redis and other resources required for Lifecycle to function properly.After the Terraform apply completes, you should have a fully functional Kubernetes cluster with the necessary resources set up.Let's test the public DNS setup by accessing the test application deployed called kuard and follow the rest of the setup instructions from the tofu apply output.\nRefer example output here to setup kubeconfig and access the cluster using kubectl.Now that your cluster is set up, you can proceed to installing Lifecycle application to your cluster.","amazon-web-services#Amazon Web Services":"*This profile needs to have access a user with AdministratorAccess access.","bootstrap-infrastructure-1#Bootstrap infrastructure":"Clone the infrastructure repository:\nFollow steps in the infrastructure repository to set up the necessary infrastructure components.\nExample secrets.auto.tfvars file:\nInitialize and apply the Terraform configuration:\nThis will create the necessary infrastructure components, including the Kubernetes cluster, DNS records, database, redis and other resources required for Lifecycle to function properly.After the Terraform apply completes, you should have a fully functional Kubernetes cluster with the necessary resources set up.Let's test the public DNS setup by accessing the test application deployed called kuard and follow the rest of the setup instructions from the tofu apply output.\nRefer example output here to setup kubeconfig and access the cluster using kubectl.Now that your cluster is set up, you can proceed to installing Lifecycle application to your cluster."}},"/docs/tips/telemetry":{"title":"Telemetry","data":{"":"Lifecycle comes with built-in support for Datadog telemetry. To collect logs and metrics from your cluster and deployed applications, install the Datadog Agent and Cluster Agent in your cluster.The deployed applications are already configured with the necessary Datadog labels and environment variables for seamless integration:Pod labels:\nEnvironment variables:\nThis setup ensures that Datadog automatically detects the environment, service, and version for each application, enabling rich observability and correlation of logs and metrics in the Datadog platform."}},"/docs/tips/using-mission-control":{"title":"Mission Control comment","data":{"":"Lifecycle uses Mission Control PR Comments to allow users to modify and customize their environments directly from the pull request comment. This enables easy service selection, branch customization, and environment variable overrides without modifying lifecycle.yaml.","selecting-and-deselecting-services#Selecting and Deselecting Services":"Each pull request environment includes default services and optional additional services. You can enable or disable services using the checkboxes.\nEnabled Services are marked with [x].\nDisabled Services are marked with [ ].\nExample:\nTo enable a service, change [ ] to [x]. To disable a service, change [x] to [ ]. As simple as that!\nIf you need to make multiple selections or deselections at once, use the\nEdit Comment option instead of clicking checkboxes individually. This\nprevents multiple back-to-back builds, as each selection triggers an event in\nLifecycle without deduplication.","choosing-a-branch#Choosing a Branch":"To deploy a specific branch for a service, modify the branch name after the service name.Example:\nThis will deploy frontend using the feature-branch instead of the default branch.","overriding-environment-variables#Overriding Environment Variables":"To set additional environment variables, use the Override Environment Variables section in the PR comment.Example:\nThis sets API_URL and CHIEF_INTERN in the environment without modifying the service configuration.","override-uuid#Override UUID":"To set a custom UUID (subdomain) for the environment, use the Override UUID section in the PR comment.\nReplace wagon-builder-060825 with your desired subdomain. This allows you to customize the environment URL without changing the underlying service configuration.Using the Mission Control PR Comment, you can easily customize your environment without modifying code, making it a flexible way to test and deploy changes dynamically."}},"/docs/troubleshooting/build-issues":{"title":"Troubleshooting Build Issues","data":{"":"TODO: This document will cover common build issues that you may encounter when\nworking with Lifecycle environments."}},"/docs/troubleshooting/deploy-issues":{"title":"Deploy Issues","data":{"":"TODO: This document will cover common deploy issues that you may encounter\nwhen working with Lifecycle environments."}},"/docs/troubleshooting/github-app-webhooks":{"title":"Missing PR comment","data":{"":"Let's quickly validate that the app is able to send events to the Lifecycle app successfully.\nNavigate to your Github app\nClick App Settings link in the Github application page\nChoose Advanced from the left sidebar\nRecent Deliveries section should show a successful delivery of the installation event to the Lifecycle app.\nIf you see an error or no deliveries, make sure the app is installed in the\natleast one repository and that the webhook URL is set correctly by\nnavigating to the General section from the left sidebar and checking the\nWebhook URL field.\nIf the delivery is successful, you should see a status code of 200 OK","failing-deliveries#Failing deliveries":"If you see a delivery failure, it could be due to various reasons. Here are some common issues and how to resolve them:","github-app-secrets#Github App secrets":"Make sure that the Github App secrets are correctly set in the lifecycle-app namespace. You can verify this by running the following command:\nThe output should include all the GITHUB_* variables with the correct values.\nIf the secrets are present but the delivery is still failing, try restarting the following deployments.\nTry triggering a new event (create a pull request) by making a change in the repository or by manually redelivering a failed delivery."}},"/docs/what-is-lifecycle":{"title":"What is Lifecycle?","data":{"":"Lifecycle is an ephemeral (/…ôÀàfem(…ô)r…ôl/, lasting for a very short time) environment orchestrator that transforms your GitHub pull requests into fully functional development environments. It enables developers to test, validate, and collaborate on features without the hassle of managing infrastructure manually.\nWith Lifecycle, every pull request gets its own connected playground‚Äîensuring that changes can be previewed, integrated, and verified before merging into its main branch.","a-developers-story#A Developer‚Äôs Story":"Imagine working in an organization that develops multiple services. Managing and testing changes across these services can be challenging, especially when multiple developers are working on different features simultaneously.Meet Nick Holiday üë®‚Äçüíª, an engineer who needs to update a database schema and modify the corresponding API in a backend service. Additionally, his change requires frontend service updates to display the new data correctly.","traditional-workflow-challenges#Traditional Workflow Challenges":"Shared environments ‚Äì Nick deploys his backend service changes to a shared dev or staging environment, but another engineer is testing unrelated changes at the same time.\nConflicting updates ‚Äì The frontend engineers working on the UI might face issues if their code depends on a stable backend service that keeps changing.\nEnvironment management ‚Äì Setting up and maintaining an isolated environment for testing requires significant effort.","enter-lifecycle#Enter Lifecycle":"With Lifecycle, as soon as Nick opens a pull request, the system automatically:\nüèóÔ∏è Creates an isolated development environment ‚Äì This environment includes Nick‚Äôs updated backend service along with the necessary frontend services.\nüöÄ Deploys the application ‚Äì Everything is set up exactly as it would be in production, ensuring a reliable test scenario.\nüîó Generates a shareable URL ‚Äì Nick and his teammates can interact with the new features without setting up anything locally.\nüßπ Cleans up automatically ‚Äì Once the PR is merged or closed, Lifecycle removes the environment, keeping things tidy.","watch-a-quick-demo#Watch a Quick Demo":"","how-it-works#How It Works":"","why-use-lifecycle#Why Use Lifecycle?":"Faster Feedback Loops - Get instant previews of your changes without waiting for staging deployments.\nIsolation - Each PR runs in its own sandbox, preventing conflicts.\nSeamless Collaboration - Share URLs with stakeholders, designers, or QA engineers.\nAutomatic Cleanup - No more stale test environments; Lifecycle manages cleanup for you.\nWorks with Your Stack - Supports containerized applications and integrates with Kubernetes."}},"/":{"title":"Lifecycle","data":{}},"/tags/aurora":{"title":"aurora","data":{}},"/tags/aws":{"title":"aws","data":{}},"/tags/auto":{"title":"auto","data":{}},"/tags/app":{"title":"app","data":{}},"/tags/automation":{"title":"automation","data":{}},"/tags/branchname":{"title":"branchname","data":{}},"/tags/build":{"title":"build","data":{}},"/tags/charts":{"title":"charts","data":{}},"/tags/builduuid":{"title":"builduuid","data":{}},"/tags/ci-cd":{"title":"ci-cd","data":{}},"/tags/cleanup":{"title":"cleanup","data":{}},"/tags/close":{"title":"close","data":{}},"/tags/cluster":{"title":"cluster","data":{}},"/tags/codefresh":{"title":"codefresh","data":{}},"/tags/comment":{"title":"comment","data":{}},"/tags/command":{"title":"command","data":{}},"/tags/config":{"title":"config","data":{}},"/tags/core":{"title":"core","data":{}},"/tags/configure":{"title":"configure","data":{}},"/tags/database":{"title":"database","data":{}},"/tags/delete":{"title":"delete","data":{}},"/tags/datadog":{"title":"datadog","data":{}},"/tags/defaultservices":{"title":"defaultservices","data":{}},"/tags/deploy":{"title":"deploy","data":{}},"/tags/deployment":{"title":"deployment","data":{}},"/tags/dev-0":{"title":"dev-0","data":{}},"/tags/docker":{"title":"docker","data":{}},"/tags/edit":{"title":"edit","data":{}},"/tags/environment variables":{"title":"environment variables","data":{}},"/tags/ephemeral-env":{"title":"ephemeral-env","data":{}},"/tags/environment":{"title":"environment","data":{}},"/tags/explore":{"title":"explore","data":{}},"/tags/error":{"title":"error","data":{}},"/tags/first environment":{"title":"first environment","data":{}},"/tags/feature-flags":{"title":"feature-flags","data":{}},"/tags/getting-started":{"title":"getting-started","data":{}},"/tags/gcp":{"title":"gcp","data":{}},"/tags/github":{"title":"github","data":{}},"/tags/glossary":{"title":"glossary","data":{}},"/tags/helm":{"title":"helm","data":{}},"/tags/internalhostname":{"title":"internalhostname","data":{}},"/tags/configuration":{"title":"configuration","data":{}},"/tags/install":{"title":"install","data":{}},"/tags/intro":{"title":"intro","data":{}},"/tags/issue":{"title":"issue","data":{}},"/tags/issues":{"title":"issues","data":{}},"/tags/kubernetes":{"title":"kubernetes","data":{}},"/tags/lifecycle-deploy":{"title":"lifecycle-deploy","data":{}},"/tags/labels":{"title":"labels","data":{}},"/tags/lifecycle-disabled":{"title":"lifecycle-disabled","data":{}},"/tags/lifecycle":{"title":"lifecycle","data":{}},"/tags/metrics":{"title":"metrics","data":{}},"/tags/lifecycle.yaml":{"title":"lifecycle.yaml","data":{}},"/tags/logs":{"title":"logs","data":{}},"/tags/missing":{"title":"missing","data":{}},"/tags/native":{"title":"native","data":{}},"/tags/mission control":{"title":"mission control","data":{}},"/tags/onboard":{"title":"onboard","data":{}},"/tags/pipeline":{"title":"pipeline","data":{}},"/tags/observability":{"title":"observability","data":{}},"/tags/optionalservices":{"title":"optionalservices","data":{}},"/tags/postgres":{"title":"postgres","data":{}},"/tags/gke":{"title":"gke","data":{}},"/tags/pr":{"title":"pr","data":{}},"/tags/redis":{"title":"redis","data":{}},"/tags/prerequisites":{"title":"prerequisites","data":{}},"/tags/publicurl":{"title":"publicurl","data":{}},"/tags/pull request":{"title":"pull request","data":{}},"/tags/requirements":{"title":"requirements","data":{}},"/tags/review":{"title":"review","data":{}},"/tags/restore":{"title":"restore","data":{}},"/tags/secrets":{"title":"secrets","data":{}},"/tags/schema":{"title":"schema","data":{}},"/tags/service-dependencies":{"title":"service-dependencies","data":{}},"/tags/services":{"title":"services","data":{}},"/tags/security":{"title":"security","data":{}},"/tags/service":{"title":"service","data":{}},"/tags/setup":{"title":"setup","data":{}},"/tags/sha":{"title":"sha","data":{}},"/tags/static":{"title":"static","data":{}},"/tags/start":{"title":"start","data":{}},"/tags/telemetry":{"title":"telemetry","data":{}},"/tags/tear down":{"title":"tear down","data":{}},"/tags/template":{"title":"template","data":{}},"/tags/term":{"title":"term","data":{}},"/tags/todo":{"title":"todo","data":{}},"/tags/terminology":{"title":"terminology","data":{}},"/tags/variables":{"title":"variables","data":{}},"/tags/webhook":{"title":"webhook","data":{}},"/tags/tutorial":{"title":"tutorial","data":{}},"/tags/uuid":{"title":"uuid","data":{}},"/tags/webhooks":{"title":"webhooks","data":{}},"/docs/features/secrets":{"title":"Cloud Secrets","data":{"overview#Overview":"Lifecycle integrates with cloud secret providers to securely inject secrets into your ephemeral environments. You can reference secrets directly in your lifecycle.yaml environment variables using a template syntax, and Lifecycle handles the rest.Supported providers:\nAWS Secrets Manager\nGCP Secret Manager","configuration#Configuration":"Secret providers are configured in the Lifecycle global_config database table under the secretProviders key.","default-configuration#Default Configuration":"Both AWS and GCP providers are enabled by default. The ClusterSecretStore created by your platform team determines which provider actually works in your cluster.\nOn AWS (EKS): Use {{aws:path:key}} syntax. The aws-secretsmanager ClusterSecretStore handles the request.\nOn GCP (GKE): Use {{gcp:path:key}} syntax. The gcp-secretmanager ClusterSecretStore handles the request.\nIf you reference a provider that doesn't have a ClusterSecretStore in your cluster, the secret will fail to sync and a warning will be logged.","configuration-fields#Configuration Fields":"Field\tRequired\tDescription\tenabled\tYes\tEnable this provider\tclusterSecretStore\tYes\tName of the ClusterSecretStore CR configured by your platform team\trefreshInterval\tYes\tHow often ESO syncs secrets (e.g., 1h, 30m)\tallowedPrefixes\tNo\tRestrict which secret paths can be referenced (empty = allow all)\t\nIf you're unsure which clusterSecretStore to use, check with your platform\nteam. They configure the External Secrets Operator and ClusterSecretStore as\npart of cluster setup.","syntax#Syntax":"Reference secrets using the following pattern:\nComponent\tDescription\tExample\tprovider\tCloud provider (aws or gcp)\taws\tsecret-path\tFull path to the secret\tmyapp/database-credentials\tkey\tJSON key within the secret (optional)\tpassword\t\nIf your secret is a plaintext value (not JSON), omit the key portion: \n  {\"{{aws:myapp/api-key}}\"}.","basic-usage#Basic Usage":"","json-secrets#JSON Secrets":"For secrets stored as JSON objects, specify the key to extract:\nIf your AWS secret myapp/rds-credentials contains:\nYour pod will receive:\nDB_USERNAME=admin\nDB_PASSWORD=supersecret\nDB_HOST=db.example.com","plaintext-secrets#Plaintext Secrets":"For secrets stored as plain strings, omit the key:","nested-json#Nested JSON":"For nested JSON structures, use dot notation:\nFor a secret containing:","where-secrets-work#Where Secrets Work":"The secret syntax works in all env blocks:","github-services#GitHub Services":"","docker-services#Docker Services":"","webhooks#Webhooks":"Helm and Codefresh deployment types do not currently support cloud secrets.","mixing-secrets-with-template-variables#Mixing Secrets with Template Variables":"You can combine cloud secrets with regular template variables in the same env block:","multiple-providers#Multiple Providers":"Both providers are enabled by default. If your platform team has configured ClusterSecretStores for both AWS and GCP, you can reference secrets from either in the same deployment:\nEach provider requires its own ClusterSecretStore. Contact your platform team\nif you need multi-cloud secret access.","error-handling#Error Handling":"Lifecycle uses a \"warn and continue\" approach for secret errors:\nScenario\tBehavior\tSecret not found\tWarning logged, env var not set, deployment continues\tKey not found in JSON\tWarning logged, env var not set\tProvider not enabled\tWarning logged, env var not set\tInvalid syntax\tValidation error at deploy time\t\nIf a required secret is missing, your application may fail to start or behave\nunexpectedly. Always verify your secrets exist before deploying.","best-practices#Best Practices":"","use-descriptive-secret-paths#Use Descriptive Secret Paths":"Organize secrets with meaningful paths:","group-related-secrets#Group Related Secrets":"Store related credentials in a single JSON secret:\nThen reference individual keys:","avoid-secrets-in-build-logs#Avoid Secrets in Build Logs":"Be cautious when using secrets in build-time environment variables, as they may appear in build logs. Prefer injecting secrets at runtime when possible.","troubleshooting#Troubleshooting":"","secret-not-injected#Secret Not Injected":"Check the secret exists in AWS Secrets Manager / GCP Secret Manager\nVerify the path matches exactly (case-sensitive)\nCheck the key name if using JSON secrets\nReview Lifecycle logs for warnings about missing secrets\nVerify provider is enabled in global configuration","syntax-errors#Syntax Errors":"Common syntax mistakes:","permission-denied#Permission Denied":"If you see permission errors:\nVerify the External Secrets Operator has access to the secret path\nCheck IAM policies allow access to the specific secret\nContact your platform team if the secret is in a restricted path","wrong-provider-for-cluster#Wrong Provider for Cluster":"If you use {{gcp:...}} on an AWS cluster (or vice versa), the ExternalSecret will fail to sync because the ClusterSecretStore doesn't exist. Check Lifecycle logs for warnings like \"ClusterSecretStore not found\".","prerequisites#Prerequisites":"For cloud secrets to work, your platform team must set up:\nComponent\tDescription\tExternal Secrets Operator\tKubernetes operator that syncs secrets from cloud providers\tClusterSecretStore\tCR that configures ESO to connect to AWS Secrets Manager or GCP Secret Manager\tIAM/Workload Identity\tPermissions for ESO to read secrets (IRSA for AWS, Workload Identity for GCP)\t\nThe clusterSecretStore value in your Lifecycle config must match the ClusterSecretStore name configured by your platform team.","related#Related":"Template Variables - Learn about other template syntax\nWebhooks - Using secrets in webhook environment variables"}}}